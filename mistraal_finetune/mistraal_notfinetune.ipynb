{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/haroonm0/localdisk/conda_envs/unsloth_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel, Trainer, TrainingArguments\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import io\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, T5EncoderModel, AdamW, get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import ast\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import init_empty_weights, infer_auto_device_map\n",
    "\n",
    "from transformers import AutoConfig\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.8: Fast Mistral patching. Transformers: 4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.553 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T09:26:42.264557Z",
     "iopub.status.busy": "2024-10-20T09:26:42.264137Z",
     "iopub.status.idle": "2024-10-20T09:26:44.243465Z",
     "shell.execute_reply": "2024-10-20T09:26:44.242210Z",
     "shell.execute_reply.started": "2024-10-20T09:26:42.264519Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fact_checks = pd.read_csv('/home/stud/haroonm0/localdisk/Fact_check/dataset/fact_checks.csv')\n",
    "posts = pd.read_csv('/home/stud/haroonm0/localdisk/Fact_check/dataset/posts.csv')\n",
    "pairs = pd.read_csv('/home/stud/haroonm0/localdisk/Fact_check/dataset/pairs.csv')\n",
    "\n",
    "\n",
    "def create_pos_neg_pairs(train_df, merged_data):\n",
    "    pos_pairs = []\n",
    "    for _, row in train_df.iterrows():\n",
    "        pos_pairs.append({\n",
    "            'post_id': row['post_id'],\n",
    "            'text_original': row['text_original'],\n",
    "            'text_translated': row['text_translated'],\n",
    "            'fact_check_id': row['fact_check_id'],\n",
    "            'original_claim': row['original_claim'],\n",
    "            'translated_claim': row['translated_claim'],\n",
    "            'label': 1  # Positive label\n",
    "        })\n",
    "\n",
    "    unique_fact_checks = merged_data.drop_duplicates(subset='fact_check_id')\n",
    "    fact_check_ids = merged_data['fact_check_id'].unique()\n",
    "\n",
    "    neg_pairs = []\n",
    "    for _, row in train_df.iterrows():\n",
    "        correct_fact_check_id = row['fact_check_id']\n",
    "        available_fact_check_ids = list(set(fact_check_ids) - {correct_fact_check_id})\n",
    "        negative_fact_checks = random.sample(available_fact_check_ids, k=min(3, len(available_fact_check_ids)))\n",
    "\n",
    "        for neg_fact_id in negative_fact_checks:\n",
    "            neg_fact_check_row = merged_data[merged_data['fact_check_id'] == neg_fact_id].iloc[0]\n",
    "            neg_pairs.append({\n",
    "                'post_id': row['post_id'],\n",
    "                'text_original': row['text_original'],\n",
    "                'text_translated': row['text_translated'],\n",
    "                'fact_check_id': neg_fact_id,\n",
    "                'original_claim': neg_fact_check_row['original_claim'],\n",
    "                'translated_claim': neg_fact_check_row['translated_claim'],\n",
    "                'label': 0  # Negative label\n",
    "            })\n",
    "\n",
    "    all_pairs = pos_pairs + neg_pairs\n",
    "    return pd.DataFrame(all_pairs)\n",
    "\n",
    "def split_data(posts, merged_data, test_size=0.2, val_size=0.5, random_state=42):\n",
    "    unique_post_ids = posts['post_id'].unique()\n",
    "    train_ids, temp_ids = train_test_split(unique_post_ids, test_size=test_size, random_state=random_state)\n",
    "    val_ids, test_ids = train_test_split(temp_ids, test_size=val_size, random_state=random_state)\n",
    "\n",
    "    train_df = merged_data[merged_data['post_id'].isin(train_ids)]\n",
    "    val_df = merged_data[merged_data['post_id'].isin(val_ids)]\n",
    "    test_df = merged_data[merged_data['post_id'].isin(test_ids)]\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def merge_data(posts, fact_checks, pairs):\n",
    "    \"\"\"\n",
    "    Merge posts and fact_checks using pairs as the bridge.\n",
    "    \"\"\"\n",
    "\n",
    "    posts = posts.drop_duplicates(subset='post_id')\n",
    "    fact_checks = fact_checks.drop_duplicates(subset='fact_check_id')\n",
    "    pairs = pairs.drop_duplicates(subset=['post_id', 'fact_check_id'])\n",
    "\n",
    "\n",
    "    merged_data = pairs.merge(posts, on='post_id', how='left').merge(fact_checks, on='fact_check_id', how='left')\n",
    "\n",
    "\n",
    "    merged_data.drop(\n",
    "        columns=['instances_x', 'verdicts', 'ocr_confidence', 'instances_y', 'confidence'],\n",
    "        inplace=True,\n",
    "        errors='ignore'\n",
    "    )\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pre_process(posts, fact_checks, pairs):\n",
    "\n",
    "\n",
    "    #fact_checks = pd.read_csv(\"/home/stud/haroonm0/localdisk/Fact_check/dataset/fact_checks.csv\")\n",
    "    #posts = pd.read_csv(\"/home/stud/haroonm0/localdisk/Fact_check/dataset/posts.csv\")\n",
    "    #pairs = pd.read_csv(\"/home/stud/haroonm0/localdisk/Fact_check/dataset/pairs.csv\")\n",
    "    preprocessor = TextPreprocessor()\n",
    "\n",
    "    if 'text' in posts.columns and 'ocr' in posts.columns:\n",
    "        posts = replace_text_with_ocr(posts, text_column='text', ocr_column='ocr')\n",
    "    if 'ocr' in posts.columns:\n",
    "        posts[['ocr_original', 'ocr_translated', 'ocr_language', 'ocr_confidence']] = posts.apply(\n",
    "            lambda row: split_text_column(row, 'ocr'), axis=1\n",
    "        )\n",
    "    if 'text' in posts.columns:\n",
    "        posts[['text_original', 'text_translated', 'text_language', 'text_confidence']] = posts.apply(\n",
    "            lambda row: split_text_column(row, 'text'), axis=1\n",
    "        )\n",
    "    fact_checks[['original_claim', 'translated_claim', 'language', 'confidence']] = fact_checks.apply(\n",
    "        lambda row: split_text_column(row, 'claim'), axis=1\n",
    "    )\n",
    "\n",
    "    columns_to_preprocess = [\n",
    "        ('translated_claim', fact_checks),\n",
    "        ('text_translated', posts),\n",
    "        ('ocr_translated', posts),\n",
    "        ('original_claim', fact_checks),\n",
    "        ('text_original', posts),\n",
    "        ('ocr_original', posts)\n",
    "    ]\n",
    "    for col, df in columns_to_preprocess:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(preprocessor.preprocess)\n",
    "\n",
    "    mergedata = merge_data(posts, fact_checks, pairs)\n",
    "    mergedata = mergedata.drop_duplicates(subset=\"translated_claim\", keep=\"first\")\n",
    "    mergedata = mergedata.drop_duplicates(subset=\"original_claim\", keep=\"first\")\n",
    "    train_df, val_df, test_df = split_data(posts, mergedata)\n",
    "\n",
    "    return train_df, test_df, val_df, mergedata\n",
    "\n",
    "\n",
    "import re\n",
    "import ast\n",
    "import re\n",
    "from collections import Counter\n",
    "import  pandas as pd\n",
    "\n",
    "class TextPreprocessor:\n",
    "    @staticmethod\n",
    "    def remove_urls(text):\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "        return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_emojis(text):\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "        emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            \"]+\",\n",
    "            flags=re.UNICODE,\n",
    "        )\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_whitespaces(text):\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        text = self.remove_urls(text)\n",
    "        text = self.remove_emojis(text)\n",
    "        text = self.replace_whitespaces(text)\n",
    "        return text\n",
    "\n",
    "# Move these functions outside of the class\n",
    "\n",
    "def replace_text_with_ocr(df, text_column='text', ocr_column='ocr'):\n",
    "    df[text_column] = df.apply(\n",
    "        lambda row: row[ocr_column] if pd.isna(row[text_column]) or row[text_column].strip() == '' else row[text_column],\n",
    "        axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def safe_literal_eval(val):\n",
    "    try:\n",
    "        if isinstance(val, str):\n",
    "            parsed_val = ast.literal_eval(val)\n",
    "            if isinstance(parsed_val, list):\n",
    "                for item in parsed_val:\n",
    "                    if isinstance(item, tuple) and len(item) == 3:\n",
    "                        return item  # Return the first valid tuple\n",
    "            elif isinstance(parsed_val, tuple) and len(parsed_val) == 3:\n",
    "                return parsed_val\n",
    "        return val\n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        print(f\"Literal eval failed for {val}: {e}\")\n",
    "        return (None, None, None)\n",
    "\n",
    "\n",
    "def split_text_column(row, row_name):\n",
    "    try:\n",
    "        parsed = safe_literal_eval(row[row_name])\n",
    "        if isinstance(parsed, tuple) and len(parsed) == 3:\n",
    "            first_text = parsed[0].strip() if isinstance(parsed[0], str) else None\n",
    "            second_text = parsed[1].strip() if isinstance(parsed[1], str) else None\n",
    "\n",
    "            # Extract the language with the highest confidence\n",
    "            if isinstance(parsed[2], list) and all(isinstance(item, tuple) for item in parsed[2]):\n",
    "                lang_conf = max(parsed[2], key=lambda x: x[1] if len(x) == 2 else 0)\n",
    "            else:\n",
    "                lang_conf = (None, None)\n",
    "\n",
    "            lang = lang_conf[0]\n",
    "            confidence = lang_conf[1]\n",
    "            return pd.Series([first_text, second_text, lang, confidence])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {row[row_name]} -> {e}\")\n",
    "    return pd.Series([None, None, None, None])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_ranking_data(examples, tokenizer, max_seq_length):\n",
    "    # Format the texts\n",
    "    formatted_texts = [\n",
    "        f\"Post: {post}\\nClaim: {claim}\" for post, claim in zip(examples[\"post\"], examples[\"claim\"])\n",
    "    ]\n",
    "\n",
    "    # Tokenize the batch\n",
    "    tokenized = tokenizer(\n",
    "        formatted_texts,\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Add labels\n",
    "    tokenized[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_ranking_data_from_pairs(pairs):\n",
    "    formatted_data = []\n",
    "    for _, row in pairs.iterrows():\n",
    "        formatted_data.append({\n",
    "            \"post\": str(row[\"text_original\"]),\n",
    "            \"claim\": str(row[\"original_claim\"]),\n",
    "            \"label\": int(row[\"label\"])\n",
    "        })\n",
    "    return formatted_data\n",
    "\n",
    "\n",
    "\n",
    "#train_data = format_ranking_data_from_pairs(train_pairs)\n",
    "\n",
    "#val_data = format_ranking_data_from_pairs(val_pairs)\n",
    "#test_data = format_ranking_data_from_pairs(test_pairs)\n",
    "\n",
    "#train_dataset = Dataset.from_list(train_data)\n",
    "#val_dataset = Dataset.from_list(val_data)\n",
    "#test_dataset = Dataset.from_list(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24431\n",
      "11386\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessing steps\n",
    "print(len(posts))\n",
    "train_df, test_df, val_df, mergedata = pre_process(posts, fact_checks, pairs)\n",
    "#train_df = train_df.sample(frac=0.5)\n",
    "#test_df = test_df.sample(frac=0.5)\n",
    "#val_df = val_df.sample(frac=0.5)\n",
    "print(len(train_df))\n",
    "\n",
    "# Generate positive and negative pairs\n",
    "train_pairs = create_pos_neg_pairs(train_df, mergedata)\n",
    "val_pairs = create_pos_neg_pairs(val_df, mergedata)\n",
    "#test_pairs = create_pos_neg_pairs(test_df, mergedata)\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_pairs)\n",
    "val_dataset = Dataset.from_pandas(val_pairs)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Shuffle datasets\n",
    "train_dataset2 = train_dataset.shuffle(seed=42)\n",
    "val_dataset2 = val_dataset.shuffle(seed=42)\n",
    "#test_dataset2 = test_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m success \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m post, original_claim \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(posts, claims):  \n\u001b[0;32m---> 63\u001b[0m     top_10_claims \u001b[38;5;241m=\u001b[39m \u001b[43mrank_claims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclaims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal claim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_claim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList of  claim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_10_claims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 44\u001b[0m, in \u001b[0;36mrank_claims\u001b[0;34m(post, claims, model, tokenizer, max_length, batch_size)\u001b[0m\n\u001b[1;32m     41\u001b[0m         logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits  \u001b[38;5;66;03m# Shape: [batch_size, seq_len, vocab_size]\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Use the last token's logits as relevance scores\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     batch_scores \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSupported\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     46\u001b[0m     scores\u001b[38;5;241m.\u001b[39mextend(batch_scores)\n\u001b[1;32m     49\u001b[0m ranked_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(scores)\u001b[38;5;241m.\u001b[39margsort(descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Descending order\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Define the Alpaca-style prompt\n",
    "alpaca_prompt = \"\"\" ### Instruction:\n",
    "Determine if the following claim is supported by the given post.\n",
    "\n",
    "### Post:\n",
    "{}\n",
    "\n",
    "### Claim:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}.\"\"\"\n",
    "\n",
    "\n",
    "def rank_claims(post, claims, model, tokenizer, max_length=512, batch_size=4):\n",
    "\n",
    "    scores = []\n",
    "    for i in range(0, len(claims), batch_size):\n",
    "        batch_claims = claims[i : i + batch_size]\n",
    "        inputs = [\n",
    "            alpaca_prompt.format(post, claim, \"\") for claim in batch_claims\n",
    "        ]\n",
    "        \n",
    "        # Tokenize the batch\n",
    "        tokenized_inputs = tokenizer(\n",
    "            inputs,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        # Get logits for the batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokenized_inputs)\n",
    "            logits = outputs.logits  # Shape: [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        # Use the last token's logits as relevance scores\n",
    "        batch_scores = logits[:, -1, tokenizer.convert_tokens_to_ids(\"Supported\")].to(dtype=torch.float32).cpu().numpy()\n",
    "\n",
    "        scores.extend(batch_scores)\n",
    "    \n",
    "    \n",
    "    ranked_indices = torch.tensor(scores).argsort(descending=True).numpy()  # Descending order\n",
    "    ranked_claims = [(claims[idx], scores[idx]) for idx in ranked_indices]\n",
    "\n",
    "    # Return top 10 claims\n",
    "    return ranked_claims[:10]\n",
    "\n",
    "\n",
    "posts = test_dataset['text_original'][:500]\n",
    "claims = test_dataset['original_claim']\n",
    "claims_orig = test_dataset['original_claim'][:500]\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "success = 0\n",
    "for post, original_claim in zip(posts, claims_orig):  \n",
    "    top_10_claims = rank_claims(post, claims, model, tokenizer, batch_size=8)\n",
    "    print(f\"original claim {original_claim}\")\n",
    "    print(f\"List of  claim {top_10_claims}\")\n",
    "    for i, (claimfound, score) in enumerate(top_10_claims, start=1):\n",
    "        if claimfound == original_claim:\n",
    "            success = success + 1\n",
    "            print(\"success\")\n",
    "print(f\"success is {success}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5911331,
     "sourceId": 9672939,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5911441,
     "sourceId": 9673076,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
