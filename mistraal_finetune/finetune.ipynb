{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import Dataset\n",
    "from preprocess import TextPreprocessor, replace_text_with_ocr, split_text_column\n",
    "from merge import merge_data\n",
    "from dataset_creation import split_data\n",
    "from preprocess import *\n",
    "from proccessclaims import *\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import *\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import logging\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from preprocess import TextPreprocessor, replace_text_with_ocr, split_text_column\n",
    "from merge import merge_data\n",
    "from dataset_creation import split_data\n",
    "from proccessclaims import create_pos_neg_pairs\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import os\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "def pre_process():\n",
    "    logging.debug(\"Starting data preprocessing.\")\n",
    "\n",
    "    fact_checks = pd.read_csv(\"/home/stud/haroonm0/localdisk/Fact_check/dataset/fact_checks.csv\")\n",
    "    posts = pd.read_csv(\"/home/stud/haroonm0/localdisk/Fact_check/dataset/posts.csv\")\n",
    "    pairs = pd.read_csv(\"/home/stud/haroonm0/localdisk/Fact_check/dataset/pairs.csv\")\n",
    "    preprocessor = TextPreprocessor()\n",
    "\n",
    "    if 'text' in posts.columns and 'ocr' in posts.columns:\n",
    "        posts = replace_text_with_ocr(posts, text_column='text', ocr_column='ocr')\n",
    "    if 'ocr' in posts.columns:\n",
    "        posts[['ocr_original', 'ocr_translated', 'ocr_language', 'ocr_confidence']] = posts.apply(\n",
    "            lambda row: split_text_column(row, 'ocr'), axis=1\n",
    "        )\n",
    "    if 'text' in posts.columns:\n",
    "        posts[['text_original', 'text_translated', 'text_language', 'text_confidence']] = posts.apply(\n",
    "            lambda row: split_text_column(row, 'text'), axis=1\n",
    "        )\n",
    "    fact_checks[['original_claim', 'translated_claim', 'language', 'confidence']] = fact_checks.apply(\n",
    "        lambda row: split_text_column(row, 'claim'), axis=1\n",
    "    )\n",
    "\n",
    "    columns_to_preprocess = [\n",
    "        ('translated_claim', fact_checks),\n",
    "        ('text_translated', posts),\n",
    "        ('ocr_translated', posts),\n",
    "        ('original_claim', fact_checks),\n",
    "        ('text_original', posts),\n",
    "        ('ocr_original', posts)\n",
    "    ]\n",
    "    for col, df in columns_to_preprocess:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(preprocessor.preprocess)\n",
    "\n",
    "    mergedata = merge_data(posts, fact_checks, pairs)\n",
    "    mergedata = mergedata.drop_duplicates(subset=\"translated_claim\", keep=\"first\")\n",
    "    mergedata = mergedata.drop_duplicates(subset=\"original_claim\", keep=\"first\")\n",
    "    train_df, val_df, test_df = split_data(posts, mergedata)\n",
    "\n",
    "    return train_df, test_df, val_df, mergedata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Starting data preprocessing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443\n",
      "DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 \"GET /unslothai/unsloth/main/unsloth/models/mapper.py HTTP/11\" 200 2595\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/adapter_config.json HTTP/11\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/commits/main HTTP/11\" 200 14204\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/commits/main?p=1 HTTP/11\" 200 5613\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unslothai/other/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:filelock:Attempting to acquire lock 139663788903680 on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--other/c0fd5260818f672a9119328c53cc33df84f30698.lock\n",
      "DEBUG:filelock:Lock 139663788903680 acquired on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--other/c0fd5260818f672a9119328c53cc33df84f30698.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.8: Fast Llama patching. Transformers: 4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.553 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /unslothai/other/resolve/main/config.json HTTP/11\" 200 635\n",
      "DEBUG:filelock:Attempting to release lock 139663788903680 on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--other/c0fd5260818f672a9119328c53cc33df84f30698.lock\n",
      "DEBUG:filelock:Lock 139663788903680 released on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--other/c0fd5260818f672a9119328c53cc33df84f30698.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unslothai/other/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:filelock:Attempting to acquire lock 139663788911552 on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--other/c0fd5260818f672a9119328c53cc33df84f30698.lock\n",
      "DEBUG:filelock:Lock 139663788911552 acquired on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--other/c0fd5260818f672a9119328c53cc33df84f30698.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /unslothai/other/resolve/main/config.json HTTP/11\" 200 635\n",
      "DEBUG:filelock:Attempting to release lock 139663788911552 on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--other/c0fd5260818f672a9119328c53cc33df84f30698.lock\n",
      "DEBUG:filelock:Lock 139663788911552 released on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--other/c0fd5260818f672a9119328c53cc33df84f30698.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unslothai/repeat/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unslothai/vram-24/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:filelock:Attempting to acquire lock 139663788904928 on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--vram-24/c0fd5260818f672a9119328c53cc33df84f30698.lock\n",
      "DEBUG:filelock:Lock 139663788904928 acquired on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--vram-24/c0fd5260818f672a9119328c53cc33df84f30698.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /unslothai/vram-24/resolve/main/config.json HTTP/11\" 200 635\n",
      "DEBUG:filelock:Attempting to release lock 139663788904928 on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--vram-24/c0fd5260818f672a9119328c53cc33df84f30698.lock\n",
      "DEBUG:filelock:Lock 139663788904928 released on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--vram-24/c0fd5260818f672a9119328c53cc33df84f30698.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unslothai/vram-24/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:filelock:Attempting to acquire lock 139663788902480 on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--vram-24/c0fd5260818f672a9119328c53cc33df84f30698.lock\n",
      "DEBUG:filelock:Lock 139663788902480 acquired on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--vram-24/c0fd5260818f672a9119328c53cc33df84f30698.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /unslothai/vram-24/resolve/main/config.json HTTP/11\" 200 635\n",
      "DEBUG:filelock:Attempting to release lock 139663788902480 on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--vram-24/c0fd5260818f672a9119328c53cc33df84f30698.lock\n",
      "DEBUG:filelock:Lock 139663788902480 released on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--vram-24/c0fd5260818f672a9119328c53cc33df84f30698.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unslothai/1/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:filelock:Attempting to acquire lock 139663788905696 on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--1/b45487c7fddc646b832426e98d1ba4da26a58eaa.lock\n",
      "DEBUG:filelock:Lock 139663788905696 acquired on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--1/b45487c7fddc646b832426e98d1ba4da26a58eaa.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /unslothai/1/resolve/main/config.json HTTP/11\" 200 638\n",
      "DEBUG:filelock:Attempting to release lock 139663788905696 on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--1/b45487c7fddc646b832426e98d1ba4da26a58eaa.lock\n",
      "DEBUG:filelock:Lock 139663788905696 released on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--1/b45487c7fddc646b832426e98d1ba4da26a58eaa.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unslothai/1/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:filelock:Attempting to acquire lock 139663788905024 on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--1/b45487c7fddc646b832426e98d1ba4da26a58eaa.lock\n",
      "DEBUG:filelock:Lock 139663788905024 acquired on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--1/b45487c7fddc646b832426e98d1ba4da26a58eaa.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /unslothai/1/resolve/main/config.json HTTP/11\" 200 638\n",
      "DEBUG:filelock:Attempting to release lock 139663788905024 on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--1/b45487c7fddc646b832426e98d1ba4da26a58eaa.lock\n",
      "DEBUG:filelock:Lock 139663788905024 released on /home/stud/haroonm0/.cache/huggingface/hub/.locks/models--unslothai--1/b45487c7fddc646b832426e98d1ba4da26a58eaa.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/generation_config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45544/45544 [00:00<00:00, 230415.90 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5688/5688 [00:00<00:00, 240074.88 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5572/5572 [00:00<00:00, 236672.12 examples/s]\n",
      "/home/stud/haroonm0/localdisk/conda_envs/unsloth_env/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45544/45544 [00:02<00:00, 19784.90 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5688/5688 [00:00<00:00, 17891.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_ranking_data(examples, tokenizer, max_seq_length):\n",
    "    # Format the texts\n",
    "    formatted_texts = [\n",
    "        f\"Post: {post}\\nClaim: {claim}\" for post, claim in zip(examples[\"post\"], examples[\"claim\"])\n",
    "    ]\n",
    "\n",
    "    # Tokenize the batch\n",
    "    tokenized = tokenizer(\n",
    "        formatted_texts,\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Add labels\n",
    "    tokenized[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def save_dataframe_to_csv(df, filename):\n",
    "    \"\"\"Save a DataFrame as a CSV file.\"\"\"\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "def load_csv_to_dataframe(filename):\n",
    "    \"\"\"Load a CSV file into a DataFrame.\"\"\"\n",
    "    if not os.path.exists(filename) or os.path.getsize(filename) == 0:\n",
    "        return None\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "\n",
    "def format_ranking_data_from_pairs(pairs):\n",
    "    formatted_data = []\n",
    "    for _, row in pairs.iterrows():\n",
    "        formatted_data.append({\n",
    "            \"post\": str(row[\"text_original\"]),\n",
    "            \"claim\": str(row[\"original_claim\"]),\n",
    "            \"label\": int(row[\"label\"])\n",
    "        })\n",
    "    return formatted_data\n",
    "\n",
    "# Preprocess data\n",
    "train_df, test_df, val_df, mergedata = pre_process()\n",
    "\n",
    "# File paths\n",
    "train_file = \"train_pairs.json\"\n",
    "val_file = \"val_pairs.json\"\n",
    "test_file = \"test_pairs.json\"\n",
    "\n",
    "# Try loading data, else generate it\n",
    "train_pairs = load_csv_to_dataframe(train_file)\n",
    "if train_pairs is None:\n",
    "    train_pairs = create_pos_neg_pairs(train_df, mergedata)\n",
    "    save_dataframe_to_csv(train_pairs, train_file)\n",
    "\n",
    "val_pairs = load_csv_to_dataframe(val_file)\n",
    "if val_pairs is None:\n",
    "    val_pairs = create_pos_neg_pairs(val_df, mergedata)\n",
    "    save_dataframe_to_csv(val_pairs, val_file)\n",
    "\n",
    "test_pairs = load_csv_to_dataframe(test_file)\n",
    "if test_pairs is None:\n",
    "    test_pairs = create_pos_neg_pairs(test_df, mergedata)\n",
    "    save_dataframe_to_csv(test_pairs, test_file)\n",
    "\n",
    "# Format data for ranking\n",
    "train_data = format_ranking_data_from_pairs(train_pairs)\n",
    "val_data = format_ranking_data_from_pairs(val_pairs)\n",
    "test_data = format_ranking_data_from_pairs(test_pairs)\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "llm_int8_enable_fp32_cpu_offload=True\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=llm_int8_enable_fp32_cpu_offload,\n",
    ")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    quantization_config=bnb_config,  # Pass the quantization config\n",
    ")\n",
    "\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "# Define the prompt template for retrieval task\n",
    "retrieval_prompt = \"\"\"Below is a post and a claim. Indicate if the claim correctly aligns with the post. Use 1 for correct and 0 for incorrect.\n",
    "\n",
    "### Post:\n",
    "{}\n",
    "\n",
    "### Claim:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # Ensure the EOS token is used to mark the end of sequences.\n",
    "\n",
    "def formatting_func(examples):\n",
    "    # Extract fields from the dataset\n",
    "    posts = examples.get(\"post\", [])\n",
    "    claims = examples.get(\"claim\", [])\n",
    "    labels = examples.get(\"label\", [])\n",
    "    \n",
    "    # Ensure all fields are lists\n",
    "    if not isinstance(posts, list):\n",
    "        posts = [posts]\n",
    "    if not isinstance(claims, list):\n",
    "        claims = [claims]\n",
    "    if not isinstance(labels, list):\n",
    "        labels = [labels]\n",
    "    \n",
    "    # Format the texts\n",
    "    texts = []\n",
    "    for post, claim, label in zip(posts, claims, labels):\n",
    "        text = retrieval_prompt.format(post, claim, label) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "\n",
    "max_seq_length = 2048\n",
    "train_dataset = train_dataset.map(formatting_func, batched=True)\n",
    "val_dataset = val_dataset.map(formatting_func, batched=True)\n",
    "test_dataset = test_dataset.map(formatting_func, batched=True)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    dataset_text_field=\"text\",  # Use the 'text' column\n",
    "    max_seq_length=max_seq_length,\n",
    "    formatting_func=None,  # Formatting is already done\n",
    "    args=TrainingArguments(\n",
    "    per_device_train_batch_size=16,  # Reduce batch size\n",
    "    gradient_accumulation_steps=16,  # Compensate for reduced batch size\n",
    "    warmup_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-5,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    evaluation_strategy=\"steps\",  # Evaluate during training\n",
    "    eval_steps=500,  # Evaluate every 50 steps\n",
    "    save_steps=500,  # Save model checkpoints every 50 steps\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
    "    load_best_model_at_end=True,  # Load the best model at the end\n",
    "    logging_dir=\"./logs\",  # Directory for logs\n",
    "    logging_strategy=\"steps\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 45,544 | Num Epochs = 10\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 32 | Total steps = 14,230\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='833' max='14230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  833/14230 2:19:29 < 37:28:42, 0.10 it/s, Epoch 0.58/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.512300</td>\n",
       "      <td>2.331392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.125100</td>\n",
       "      <td>2.033740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.028300</td>\n",
       "      <td>1.992980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.017500</td>\n",
       "      <td>1.956154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.988800</td>\n",
       "      <td>1.918028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.962900</td>\n",
       "      <td>1.906777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.925600</td>\n",
       "      <td>1.900904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.914000</td>\n",
       "      <td>1.895901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.992400</td>\n",
       "      <td>1.891156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.880200</td>\n",
       "      <td>1.887724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.917100</td>\n",
       "      <td>1.884976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.961800</td>\n",
       "      <td>1.881226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.870200</td>\n",
       "      <td>1.877765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.922600</td>\n",
       "      <td>1.875680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.903100</td>\n",
       "      <td>1.872763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.890500</td>\n",
       "      <td>1.869976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit/resolve/main/config.json HTTP/11\" 200 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:157\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m<string>:380\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:64\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
      "File \u001b[0;32m~/localdisk/conda_envs/unsloth_env/lib/python3.10/site-packages/accelerate/accelerator.py:2248\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2248\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/localdisk/conda_envs/unsloth_env/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/localdisk/conda_envs/unsloth_env/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/localdisk/conda_envs/unsloth_env/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
