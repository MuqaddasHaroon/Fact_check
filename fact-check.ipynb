{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv('/Users/muqaddasharoon/Downloads/Thesis/fact checking claims/dataset/posts.csv')\n",
    "fact_checks = pd.read_csv('/Users/muqaddasharoon/Downloads/Thesis/fact checking claims/dataset/fact_checks.csv')\n",
    "pairs = pd.read_csv('/Users/muqaddasharoon/Downloads/Thesis/fact checking claims/dataset/pairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_checks = fact_checks.sample(frac=0.1, random_state=42)\n",
    "posts = posts.sample(frac=0.1, random_state=42)\n",
    "pairs = pairs.sample(frac=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>instances</th>\n",
       "      <th>ocr</th>\n",
       "      <th>verdicts</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2166</th>\n",
       "      <td>2398</td>\n",
       "      <td>[(1643041167.0, 'ig')]</td>\n",
       "      <td>[(\"or years the majority of people wer- onfuse...</td>\n",
       "      <td>['False']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10333</th>\n",
       "      <td>11530</td>\n",
       "      <td>[(1643829935.0, 'fb')]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Altered photo']</td>\n",
       "      <td>('Hoy, a muy tempranas horas, el Papa Francisc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10835</th>\n",
       "      <td>12081</td>\n",
       "      <td>[(1627043545.0, 'fb')]</td>\n",
       "      <td>[('Calendar week 150 8601 Number of specimens ...</td>\n",
       "      <td>['Partly false information.']</td>\n",
       "      <td>('Influenza, finally eradicated üëè', 'Influenza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4688</th>\n",
       "      <td>5218</td>\n",
       "      <td>[(1630043463.0, 'fb')]</td>\n",
       "      <td>[('Par√°metros: HOLISTIC HEALING INSTITUT MEDIZ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>('ANALISIS DE UNA MUESTRA DE AGUA DEL GRIFO DE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17560</th>\n",
       "      <td>19804</td>\n",
       "      <td>[(1580851455.0, 'fb')]</td>\n",
       "      <td>[('PER√ö Ministerio de Salud INSTITUTO NACIONAL...</td>\n",
       "      <td>['False information']</td>\n",
       "      <td>('Tomemos en cuenta el Desconocimiento de much...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id               instances  \\\n",
       "2166      2398  [(1643041167.0, 'ig')]   \n",
       "10333    11530  [(1643829935.0, 'fb')]   \n",
       "10835    12081  [(1627043545.0, 'fb')]   \n",
       "4688      5218  [(1630043463.0, 'fb')]   \n",
       "17560    19804  [(1580851455.0, 'fb')]   \n",
       "\n",
       "                                                     ocr  \\\n",
       "2166   [(\"or years the majority of people wer- onfuse...   \n",
       "10333                                                 []   \n",
       "10835  [('Calendar week 150 8601 Number of specimens ...   \n",
       "4688   [('Par√°metros: HOLISTIC HEALING INSTITUT MEDIZ...   \n",
       "17560  [('PER√ö Ministerio de Salud INSTITUTO NACIONAL...   \n",
       "\n",
       "                            verdicts  \\\n",
       "2166                       ['False']   \n",
       "10333              ['Altered photo']   \n",
       "10835  ['Partly false information.']   \n",
       "4688                              []   \n",
       "17560          ['False information']   \n",
       "\n",
       "                                                    text  \n",
       "2166                                                 NaN  \n",
       "10333  ('Hoy, a muy tempranas horas, el Papa Francisc...  \n",
       "10835  ('Influenza, finally eradicated üëè', 'Influenza...  \n",
       "4688   ('ANALISIS DE UNA MUESTRA DE AGUA DEL GRIFO DE...  \n",
       "17560  ('Tomemos en cuenta el Desconocimiento de much...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "posts = posts.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fact_check_id</th>\n",
       "      <th>claim</th>\n",
       "      <th>instances</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131579</th>\n",
       "      <td>155830</td>\n",
       "      <td>('¬´ On aide tr√®s peu les paysans bio ¬ª', '\"We ...</td>\n",
       "      <td>[(1645528774.0, 'https://www.mediacites.fr/ver...</td>\n",
       "      <td>('¬´ On aide tr√®s peu les paysans bio ¬ª', '\"We ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36821</th>\n",
       "      <td>42162</td>\n",
       "      <td>('Dado que ning√∫n fabricante de las vacunas ap...</td>\n",
       "      <td>[(1639007940.0, 'https://www.newtral.es/autori...</td>\n",
       "      <td>('La autorizaci√≥n de comercializaci√≥n de las v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141844</th>\n",
       "      <td>173033</td>\n",
       "      <td>('ŸÅŸäÿØŸäŸà ŸÑŸÑŸÖÿ≥ÿ¨ÿØ ÿßŸÑŸÜÿ®ŸàŸä ŸàŸáŸà ŸÖÿ∫ÿ∑Ÿâ ÿ®ÿßŸÑÿ´ŸÑŸàÿ¨.', \"A v...</td>\n",
       "      <td>[(1579534680.0, 'https://misbar.com/factcheck/...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45892</th>\n",
       "      <td>52785</td>\n",
       "      <td>('En lugar de dejar de usar las aplicaciones d...</td>\n",
       "      <td>[(1658534280.0, 'https://www.univision.com/not...</td>\n",
       "      <td>('Apps de menstruaci√≥n de Europa: lo que sabem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>3543</td>\n",
       "      <td>('\"Milion√°rio compra clube chin√™s e exige que ...</td>\n",
       "      <td>[(1622681063.0, 'https://observador.pt/factche...</td>\n",
       "      <td>('Fact Check. Milion√°rio chin√™s obrigou clube ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        fact_check_id                                              claim  \\\n",
       "131579         155830  ('¬´ On aide tr√®s peu les paysans bio ¬ª', '\"We ...   \n",
       "36821           42162  ('Dado que ning√∫n fabricante de las vacunas ap...   \n",
       "141844         173033  ('ŸÅŸäÿØŸäŸà ŸÑŸÑŸÖÿ≥ÿ¨ÿØ ÿßŸÑŸÜÿ®ŸàŸä ŸàŸáŸà ŸÖÿ∫ÿ∑Ÿâ ÿ®ÿßŸÑÿ´ŸÑŸàÿ¨.', \"A v...   \n",
       "45892           52785  ('En lugar de dejar de usar las aplicaciones d...   \n",
       "3430             3543  ('\"Milion√°rio compra clube chin√™s e exige que ...   \n",
       "\n",
       "                                                instances  \\\n",
       "131579  [(1645528774.0, 'https://www.mediacites.fr/ver...   \n",
       "36821   [(1639007940.0, 'https://www.newtral.es/autori...   \n",
       "141844  [(1579534680.0, 'https://misbar.com/factcheck/...   \n",
       "45892   [(1658534280.0, 'https://www.univision.com/not...   \n",
       "3430    [(1622681063.0, 'https://observador.pt/factche...   \n",
       "\n",
       "                                                    title  \n",
       "131579  ('¬´ On aide tr√®s peu les paysans bio ¬ª', '\"We ...  \n",
       "36821   ('La autorizaci√≥n de comercializaci√≥n de las v...  \n",
       "141844                                                NaN  \n",
       "45892   ('Apps de menstruaci√≥n de Europa: lo que sabem...  \n",
       "3430    ('Fact Check. Milion√°rio chin√™s obrigou clube ...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_checks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_checks['title'].fillna(fact_checks['claim'].apply(lambda x: x[:30] if isinstance(x, str) else \"No Title\"), inplace = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fact_check_id</th>\n",
       "      <th>claim</th>\n",
       "      <th>instances</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131579</th>\n",
       "      <td>155830</td>\n",
       "      <td>('¬´ On aide tr√®s peu les paysans bio ¬ª', '\"We ...</td>\n",
       "      <td>[(1645528774.0, 'https://www.mediacites.fr/ver...</td>\n",
       "      <td>('¬´ On aide tr√®s peu les paysans bio ¬ª', '\"We ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36821</th>\n",
       "      <td>42162</td>\n",
       "      <td>('Dado que ning√∫n fabricante de las vacunas ap...</td>\n",
       "      <td>[(1639007940.0, 'https://www.newtral.es/autori...</td>\n",
       "      <td>('La autorizaci√≥n de comercializaci√≥n de las v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141844</th>\n",
       "      <td>173033</td>\n",
       "      <td>('ŸÅŸäÿØŸäŸà ŸÑŸÑŸÖÿ≥ÿ¨ÿØ ÿßŸÑŸÜÿ®ŸàŸä ŸàŸáŸà ŸÖÿ∫ÿ∑Ÿâ ÿ®ÿßŸÑÿ´ŸÑŸàÿ¨.', \"A v...</td>\n",
       "      <td>[(1579534680.0, 'https://misbar.com/factcheck/...</td>\n",
       "      <td>('ŸÅŸäÿØŸäŸà ŸÑŸÑŸÖÿ≥ÿ¨ÿØ ÿßŸÑŸÜÿ®ŸàŸä ŸàŸáŸà ŸÖÿ∫ÿ∑Ÿâ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45892</th>\n",
       "      <td>52785</td>\n",
       "      <td>('En lugar de dejar de usar las aplicaciones d...</td>\n",
       "      <td>[(1658534280.0, 'https://www.univision.com/not...</td>\n",
       "      <td>('Apps de menstruaci√≥n de Europa: lo que sabem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>3543</td>\n",
       "      <td>('\"Milion√°rio compra clube chin√™s e exige que ...</td>\n",
       "      <td>[(1622681063.0, 'https://observador.pt/factche...</td>\n",
       "      <td>('Fact Check. Milion√°rio chin√™s obrigou clube ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        fact_check_id                                              claim  \\\n",
       "131579         155830  ('¬´ On aide tr√®s peu les paysans bio ¬ª', '\"We ...   \n",
       "36821           42162  ('Dado que ning√∫n fabricante de las vacunas ap...   \n",
       "141844         173033  ('ŸÅŸäÿØŸäŸà ŸÑŸÑŸÖÿ≥ÿ¨ÿØ ÿßŸÑŸÜÿ®ŸàŸä ŸàŸáŸà ŸÖÿ∫ÿ∑Ÿâ ÿ®ÿßŸÑÿ´ŸÑŸàÿ¨.', \"A v...   \n",
       "45892           52785  ('En lugar de dejar de usar las aplicaciones d...   \n",
       "3430             3543  ('\"Milion√°rio compra clube chin√™s e exige que ...   \n",
       "\n",
       "                                                instances  \\\n",
       "131579  [(1645528774.0, 'https://www.mediacites.fr/ver...   \n",
       "36821   [(1639007940.0, 'https://www.newtral.es/autori...   \n",
       "141844  [(1579534680.0, 'https://misbar.com/factcheck/...   \n",
       "45892   [(1658534280.0, 'https://www.univision.com/not...   \n",
       "3430    [(1622681063.0, 'https://observador.pt/factche...   \n",
       "\n",
       "                                                    title  \n",
       "131579  ('¬´ On aide tr√®s peu les paysans bio ¬ª', '\"We ...  \n",
       "36821   ('La autorizaci√≥n de comercializaci√≥n de las v...  \n",
       "141844                     ('ŸÅŸäÿØŸäŸà ŸÑŸÑŸÖÿ≥ÿ¨ÿØ ÿßŸÑŸÜÿ®ŸàŸä ŸàŸáŸà ŸÖÿ∫ÿ∑Ÿâ  \n",
       "45892   ('Apps de menstruaci√≥n de Europa: lo que sabem...  \n",
       "3430    ('Fact Check. Milion√°rio chin√™s obrigou clube ...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_checks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "post_ids_in_posts = set(posts['post_id'].unique())\n",
    "post_ids_in_pairs = set(pairs['post_id'].unique())\n",
    "\n",
    "\n",
    "missing_post_ids = post_ids_in_pairs - post_ids_in_posts #we are finding pairs that are  in posts.csv but not pairs\n",
    "pairs2 = pairs[~pairs['post_id'].isin(missing_post_ids)]\n",
    "\n",
    "fact_check_ids_in_fact_checks = set(fact_checks['fact_check_id'].unique())\n",
    "fact_check_ids_in_pairs = set(pairs['fact_check_id'].unique())\n",
    "\n",
    "missing_fact_check_ids = fact_check_ids_in_pairs - fact_check_ids_in_fact_checks\n",
    "\n",
    "filtered_pairs = pairs[\n",
    "    ~pairs['post_id'].isin(missing_post_ids) & \n",
    "    ~pairs['fact_check_id'].isin(missing_fact_check_ids)\n",
    "]\n",
    "\n",
    "merged_data = filtered_pairs.merge(posts, on='post_id', how='left')\n",
    "training_data = merged_data.merge(fact_checks, on='fact_check_id', how='left')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Positive Pairs: 27\n",
      "Total Negative Samples: 27\n",
      "{'post_id': 23023, 'fact_check_id': 108741, 'instances_x': \"[(1570019069.0, 'fb')]\", 'ocr': '[(\"Gerard Depardieu #pourquoi la Tunisie? 40 ? Pourquoi la Tunisie Je ne suis pas un politicien et je d√©teste les politiciens, car la plupart d\\'entre eux sont des hypocrites corrompus et flatteurs. Encore une fois, ces politiciens interviennent en Tunisie et aident les corrompus comme eux: pourquoi ne laissez-vous pas le peuple tunisien d√©cider de son sort? Pour vivre comme les autres peuples du monde, pourquoi insistez-vous pour faire de la Tunisie la gueule de vos d√©chets et un foyer de personnes corrompues? Quittez les peuples et quittez leur pays d\\'origine afin que nous ne payions pas vos erreurs dans nos pays d\\'origine. Je vais garder cette fougue odieuse vous jamais.\", \"G√©rard Depardieu #why Tunisia? 40 ? Why Tunisia I am not a politician and I hate politicians because most of them they are corrupt and flattering hypocrites. Again, these politicians intervene in Tunisia and help the corrupt like them: why don\\'t you let the Tunisian people decide their fate? For live like the other peoples of the world, why do you insist on make Tunisia the mouth of your waste and a home of people corrupt? Leave the peoples and leave their country of origin so that we don\\'t pay for your mistakes in our home countries. I will keep this hateful passion from you forever.\", [(\\'fra\\', 0.9970784783363342)])]', 'verdicts': \"['False information']\", 'text': '(\\'ÿßŸÑŸÖŸÖÿ´ŸÑ ÿßŸÑŸÅÿ±ŸÜÿ≥Ÿä \" Gerard Depardieu\" ŸÜÿ∑ŸÇ Ÿàÿßÿπÿ™ÿ±ŸÅ ÿßŸÑŸä ŸÅÿ±ŸÜÿ≥ÿß ÿ™ÿ™ÿØÿÆŸÑ ŸÅŸä ÿ¥ÿ§ŸàŸÜ ÿßŸÑÿ¥ÿπŸàÿ® Ÿàÿ™ŸàŸÜÿ≥ ŸàŸáÿ∞ÿß ÿ™ÿµÿ±Ÿäÿ≠Ÿá: \"ŸÑÿ≥ÿ™ ÿ≥Ÿäÿßÿ≥Ÿäÿß Ÿàÿ£ŸÉÿ±Ÿá ÿßŸÑÿ≥Ÿäÿßÿ≥ŸäŸäŸÜ ŸÑÿ£ŸÜ ÿ£ÿ∫ŸÑÿ®ŸáŸÖ ŸÅÿßÿ≥ÿØŸàŸÜ ŸÖŸÜÿßŸÅŸÇŸàŸÜ ŸÖÿ™ŸÖŸÑŸÇŸàŸÜ ÿ™ÿ±ŸÉÿ™ ŸÅÿ±ŸÜÿ≥ÿß ŸÖŸÜ ÿ£ÿ¨ŸÑ ÿ∞ŸÑŸÉ ŸàŸáÿ¨ÿ±ÿ™Ÿáÿß ÿ≠ÿ™Ÿâ ŸÑÿß ÿ£ÿ±Ÿâ ÿ™ŸÑŸÉ ÿßŸÑŸàÿ¨ŸàŸá ÿßŸÑŸÇÿ®Ÿäÿ≠ÿ©ÿå ÿ´ŸÖ ŸÖÿ¨ÿØÿØÿß Ÿáÿ§ŸÑÿßÿ° ÿßŸÑÿ≥Ÿäÿßÿ≥ŸäŸäŸÜ Ÿäÿ™ÿØÿÆŸÑŸàŸÜ ŸÅŸä ÿ™ŸàŸÜÿ≥ ŸàŸäÿ≥ÿßÿπÿØŸàŸÜ ÿßŸÑŸÅÿßÿ≥ÿØŸäŸÜ ÿ£ŸÖÿ´ÿßŸÑŸáŸÖ ŸÑŸÖÿßÿ∞ÿß ŸÑÿß ÿ™ÿ™ÿ±ŸÉŸàŸÜ ÿßŸÑÿ¥ÿπÿ® ÿßŸÑÿ™ŸàŸÜÿ≥Ÿä ŸäŸÇÿ±ÿ± ŸÖÿµŸäÿ±Ÿáÿü ÿ≠ÿ™Ÿâ ŸäÿπŸäÿ¥ ŸÉÿ®ÿßŸÇŸä ÿ¥ÿπŸàÿ® ÿßŸÑÿπÿßŸÑŸÖ ŸÑŸÖÿßÿ∞ÿß ÿ™ÿµÿ±ŸàŸÜ ÿπŸÑŸâ ÿ¨ÿπŸÑ ÿ™ŸàŸÜÿ≥ ŸÖÿµÿ®ÿß ŸÑŸÅÿ∂ŸÑÿßÿ™ŸÉŸÖ ŸàŸÖÿ±ÿ™ÿπÿß ŸÑŸÑŸÅÿßÿ≥ÿØŸäŸÜÿüÿü ÿßÿ™ÿ±ŸÉŸàÿß ÿßŸÑÿ¥ÿπŸàÿ® Ÿàÿßÿ±ÿ≠ŸÑŸàÿß ŸÖŸÜ ÿ£Ÿàÿ∑ÿßŸÜŸáŸÖ ÿ≠ÿ™Ÿâ ŸÑÿß ŸÜÿØŸÅÿπ ÿ´ŸÖŸÜ ÿ£ÿÆÿ∑ÿßÿ¶ŸÉŸÖ ŸÅŸä ÿ£Ÿàÿ∑ÿßŸÜŸÜÿß.. ÿ≥ÿ£ÿ®ŸÇŸâ ÿ∞ŸÑŸÉ ÿßŸÑŸÖÿ¥ÿßŸÉÿ≥ ÿßŸÑÿ®ÿ∫Ÿäÿ∂ ŸÑŸÉŸÖ ÿ£ÿ®ÿØÿß.\"\\', \\'The French actor \"Gerard Depardieu\" spoke and admitted to France interfering in the affairs of peoples and Tunisia, and this is his statement: \"I am not a politician, and I hate politicians because most of them are corrupt, hypocritical, and sycophants. I left France for this and emigrated so that I would not see those ugly faces. Then again, these politicians interfere in Tunisia and help corrupt people like them. Why don\\\\\\'t you let the Tunisian people decide their fate? So that they live like the rest of the world\\\\\\'s peoples. Why do you insist on making Tunisia is a dumping ground for your waste and a hotbed for the corrupt?? Leave the peoples and leave their countries so that we do not pay the price for your mistakes in our countries.\\', [(\\'ara\\', 1.0)])', 'claim': '(\"President Biden said the US didn\\'t have a Covid Vaccine until he took office\", \"President Biden said the US didn\\'t have a Covid Vaccine until he took office\", [(\\'eng\\', 1.0)])', 'instances_y': \"[(1613761873.0, 'https://leadstories.com/hoax-alert/2021/02/biden-did-not-say-us-lacked-covid-vaccine-before-he-took-office.html#d4b2f553782bb023360f52db10def12f')]\", 'title': \"('Fact Check: President Biden Quote About Not Having Covid-19 Vaccine Coming Into Office Was About Supply Backlog -- NOT About Vaccine Existence', 'Fact Check: President Biden Quote About Not Having Covid-19 Vaccine Coming Into Office Was About Supply Backlog -- NOT About Vaccine Existence', [('eng', 1.0)])\", 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Create positive pairs list\n",
    "pos_pairs = []\n",
    "for _, row in training_data.iterrows():\n",
    "    pair_dict = row.to_dict()\n",
    "    pair_dict['label'] = 1  # Positive label\n",
    "    pos_pairs.append(pair_dict)\n",
    "\n",
    "print(f\"Total Positive Pairs: {len(pos_pairs)}\")\n",
    "\n",
    "\n",
    "fact_check_dict = fact_checks.set_index('fact_check_id').to_dict('index') \n",
    "\n",
    "\n",
    "neg_pairs = []\n",
    "for _, row in training_data.iterrows():\n",
    "    post_text = row['text']\n",
    "    correct_fact_check_id = row['fact_check_id']\n",
    "\n",
    "    while True:\n",
    "        random_fact_id = random.choice(list(fact_check_dict.keys()))\n",
    "        if random_fact_id != correct_fact_check_id:\n",
    "            break\n",
    "\n",
    "    random_fact_check_data = fact_check_dict[random_fact_id]\n",
    "\n",
    "  \n",
    "    if pd.notna(post_text) and pd.notna(random_fact_check_data['claim']):\n",
    "        neg_pairs.append({\n",
    "            'post_id': row['post_id'],  \n",
    "            'fact_check_id': random_fact_id,  \n",
    "            'instances_x': row['instances_x'],  \n",
    "            'ocr': row['ocr'], \n",
    "            'verdicts': row['verdicts'], \n",
    "            'text': post_text,  \n",
    "            'claim': random_fact_check_data['claim'], \n",
    "            'instances_y': random_fact_check_data['instances'],\n",
    "            'title': random_fact_check_data['title'],  \n",
    "            'label': 0  \n",
    "        })\n",
    "\n",
    "print(f\"Total Negative Samples: {len(neg_pairs)}\")\n",
    "print(neg_pairs[0]) \n",
    "all_pairs = pos_pairs + neg_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_pairs)\n",
    "df = df.drop(columns = ['post_id', 'fact_check_id', 'instances_x','instances_y' ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ocr</th>\n",
       "      <th>verdicts</th>\n",
       "      <th>text</th>\n",
       "      <th>claim</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(\"Gerard Depardieu #pourquoi la Tunisie? 40 ?...</td>\n",
       "      <td>['False information']</td>\n",
       "      <td>('ÿßŸÑŸÖŸÖÿ´ŸÑ ÿßŸÑŸÅÿ±ŸÜÿ≥Ÿä \" Gerard Depardieu\" ŸÜÿ∑ŸÇ Ÿàÿßÿπÿ™ÿ±...</td>\n",
       "      <td>('ÿßŸÑŸÖŸÖÿ´ŸÑ ÿßŸÑŸÅÿ±ŸÜÿ≥Ÿä Gerard Depardieu ŸÜÿ∑ŸÇ Ÿàÿßÿπÿ™ÿ±ŸÅ ÿ£...</td>\n",
       "      <td>('Ÿáÿ∞Ÿá ÿßŸÑÿµŸàÿ±ÿ© ŸÑŸÑŸÖŸÖÿ´ŸëŸÑ ÿ¨Ÿäÿ±ÿßÿ± ÿØŸàÿ®ÿßÿ±ÿØŸäŸà ŸÖÿ±ŸÉŸëÿ®ÿ©', '...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('MARVIN BERGAUER REPORTER OE24TV WIEN: DEMO ...</td>\n",
       "      <td>['Missing context']</td>\n",
       "      <td>('üòÇ Milagros del cambio pland√©mico... üëá', 'üòÇ M...</td>\n",
       "      <td>('V√≠deo de un informativo en el que un cad√°ver...</td>\n",
       "      <td>('No, este v√≠deo de un informativo austriaco e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>('GRAN FRASE ü§∫ ‚ÄúQuerido Sancho: Compruebo con ...</td>\n",
       "      <td>('Sabio Don Miguel de Cervantes. ‚ÄúQuerido Sanc...</td>\n",
       "      <td>(\"Querido Sancho compruebo con pesar...: la ci...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>['Missing context.']</td>\n",
       "      <td>('Tarian klasik ini baru diciptakan di China d...</td>\n",
       "      <td>('Video penari robot buatan Tiongkok tampil di...</td>\n",
       "      <td>(\"Penari di video ini adalah manusia, bukan 'r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>['False information']</td>\n",
       "      <td>('‡§ú‡§æ‡§´‡§º‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§®‡§à ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§ï‡•á ‡§¶‡•É‡§∂‡•ç‡§Ø.. ‡§≤‡§ó‡§§‡§æ ‡§π‡•à ‡§™‡•ç‡§∞‡§ß‡§æ...</td>\n",
       "      <td>(\"Video shows COVID protocol flouted in Delhi'...</td>\n",
       "      <td>('Video From Lahore Passed Off as COVID-19 Nor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ocr               verdicts  \\\n",
       "0  [(\"Gerard Depardieu #pourquoi la Tunisie? 40 ?...  ['False information']   \n",
       "1  [('MARVIN BERGAUER REPORTER OE24TV WIEN: DEMO ...    ['Missing context']   \n",
       "2                                                 []                     []   \n",
       "3                                                 []   ['Missing context.']   \n",
       "4                                                 []  ['False information']   \n",
       "\n",
       "                                                text  \\\n",
       "0  ('ÿßŸÑŸÖŸÖÿ´ŸÑ ÿßŸÑŸÅÿ±ŸÜÿ≥Ÿä \" Gerard Depardieu\" ŸÜÿ∑ŸÇ Ÿàÿßÿπÿ™ÿ±...   \n",
       "1  ('üòÇ Milagros del cambio pland√©mico... üëá', 'üòÇ M...   \n",
       "2  ('GRAN FRASE ü§∫ ‚ÄúQuerido Sancho: Compruebo con ...   \n",
       "3  ('Tarian klasik ini baru diciptakan di China d...   \n",
       "4  ('‡§ú‡§æ‡§´‡§º‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§®‡§à ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§ï‡•á ‡§¶‡•É‡§∂‡•ç‡§Ø.. ‡§≤‡§ó‡§§‡§æ ‡§π‡•à ‡§™‡•ç‡§∞‡§ß‡§æ...   \n",
       "\n",
       "                                               claim  \\\n",
       "0  ('ÿßŸÑŸÖŸÖÿ´ŸÑ ÿßŸÑŸÅÿ±ŸÜÿ≥Ÿä Gerard Depardieu ŸÜÿ∑ŸÇ Ÿàÿßÿπÿ™ÿ±ŸÅ ÿ£...   \n",
       "1  ('V√≠deo de un informativo en el que un cad√°ver...   \n",
       "2  ('Sabio Don Miguel de Cervantes. ‚ÄúQuerido Sanc...   \n",
       "3  ('Video penari robot buatan Tiongkok tampil di...   \n",
       "4  (\"Video shows COVID protocol flouted in Delhi'...   \n",
       "\n",
       "                                               title  label  \n",
       "0  ('Ÿáÿ∞Ÿá ÿßŸÑÿµŸàÿ±ÿ© ŸÑŸÑŸÖŸÖÿ´ŸëŸÑ ÿ¨Ÿäÿ±ÿßÿ± ÿØŸàÿ®ÿßÿ±ÿØŸäŸà ŸÖÿ±ŸÉŸëÿ®ÿ©', '...      1  \n",
       "1  ('No, este v√≠deo de un informativo austriaco e...      1  \n",
       "2  (\"Querido Sancho compruebo con pesar...: la ci...      1  \n",
       "3  (\"Penari di video ini adalah manusia, bukan 'r...      1  \n",
       "4  ('Video From Lahore Passed Off as COVID-19 Nor...      1  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, EncoderDecoderModel, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "\n",
    "def preprocess_data(ocr, verdicts, text):\n",
    "    combined_input = ocr + \" [SEP] \" + verdicts + \" [SEP] \" + text\n",
    "    return combined_input\n",
    "\n",
    "# Concatenate relevant information for both train and test\n",
    "train_inputs = [preprocess_data(a, b, c) for a, b, c in zip(train_df['ocr'], train_df['verdicts'], train_df['text'])]\n",
    "test_inputs = [preprocess_data(a, b, c) for a, b, c in zip(test_df['ocr'], test_df['verdicts'], test_df['text'])]\n",
    "\n",
    "# Tokenize input text (input sentences) and output text (claims)\n",
    "train_encodings = tokenizer(train_inputs, padding=\"max_length\", truncation=True, max_length=128, return_tensors='pt')\n",
    "test_encodings = tokenizer(test_inputs, padding=\"max_length\", truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Tokenize the claims (outputs)\n",
    "train_claims_encodings = tokenizer(train_df['claim'].tolist(), padding=\"max_length\", truncation=True, max_length=128, return_tensors='pt')\n",
    "test_claims_encodings = tokenizer(test_df['claim'].tolist(), padding=\"max_length\", truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels['input_ids'][idx]  # Use the input_ids of the target (claim) as labels\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels['input_ids'])\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, train_claims_encodings)\n",
    "test_dataset = CustomDataset(test_encodings, test_claims_encodings)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/muqaddasharoon/opt/anaconda3/envs/lab/lib/python3.11/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:623: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/Users/muqaddasharoon/opt/anaconda3/envs/lab/lib/python3.11/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 11.256892999013266\n",
      "Epoch 2/3, Loss: 6.579164028167725\n",
      "Epoch 3/3, Loss: 4.618646105130513\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, EncoderDecoderModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-multilingual-cased', 'bert-base-multilingual-cased')\n",
    "\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_train_loss}\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Implement your evaluation loop here if needed\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
