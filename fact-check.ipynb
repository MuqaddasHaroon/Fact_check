{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv('/Users/muqaddasharoon/Downloads/Thesis/fact checking claims/dataset/posts.csv')\n",
    "fact_checks = pd.read_csv('/Users/muqaddasharoon/Downloads/Thesis/fact checking claims/dataset/fact_checks.csv')\n",
    "pairs = pd.read_csv('/Users/muqaddasharoon/Downloads/Thesis/fact checking claims/dataset/pairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_checks = fact_checks.sample(frac=0.1, random_state=42)\n",
    "posts = posts.sample(frac=0.1, random_state=42)\n",
    "pairs = pairs.sample(frac=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>instances</th>\n",
       "      <th>ocr</th>\n",
       "      <th>verdicts</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2166</th>\n",
       "      <td>2398</td>\n",
       "      <td>[(1643041167.0, 'ig')]</td>\n",
       "      <td>[(\"or years the majority of people wer- onfuse...</td>\n",
       "      <td>['False']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10333</th>\n",
       "      <td>11530</td>\n",
       "      <td>[(1643829935.0, 'fb')]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Altered photo']</td>\n",
       "      <td>('Hoy, a muy tempranas horas, el Papa Francisc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10835</th>\n",
       "      <td>12081</td>\n",
       "      <td>[(1627043545.0, 'fb')]</td>\n",
       "      <td>[('Calendar week 150 8601 Number of specimens ...</td>\n",
       "      <td>['Partly false information.']</td>\n",
       "      <td>('Influenza, finally eradicated ğŸ‘', 'Influenza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4688</th>\n",
       "      <td>5218</td>\n",
       "      <td>[(1630043463.0, 'fb')]</td>\n",
       "      <td>[('ParÃ¡metros: HOLISTIC HEALING INSTITUT MEDIZ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>('ANALISIS DE UNA MUESTRA DE AGUA DEL GRIFO DE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17560</th>\n",
       "      <td>19804</td>\n",
       "      <td>[(1580851455.0, 'fb')]</td>\n",
       "      <td>[('PERÃš Ministerio de Salud INSTITUTO NACIONAL...</td>\n",
       "      <td>['False information']</td>\n",
       "      <td>('Tomemos en cuenta el Desconocimiento de much...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id               instances  \\\n",
       "2166      2398  [(1643041167.0, 'ig')]   \n",
       "10333    11530  [(1643829935.0, 'fb')]   \n",
       "10835    12081  [(1627043545.0, 'fb')]   \n",
       "4688      5218  [(1630043463.0, 'fb')]   \n",
       "17560    19804  [(1580851455.0, 'fb')]   \n",
       "\n",
       "                                                     ocr  \\\n",
       "2166   [(\"or years the majority of people wer- onfuse...   \n",
       "10333                                                 []   \n",
       "10835  [('Calendar week 150 8601 Number of specimens ...   \n",
       "4688   [('ParÃ¡metros: HOLISTIC HEALING INSTITUT MEDIZ...   \n",
       "17560  [('PERÃš Ministerio de Salud INSTITUTO NACIONAL...   \n",
       "\n",
       "                            verdicts  \\\n",
       "2166                       ['False']   \n",
       "10333              ['Altered photo']   \n",
       "10835  ['Partly false information.']   \n",
       "4688                              []   \n",
       "17560          ['False information']   \n",
       "\n",
       "                                                    text  \n",
       "2166                                                 NaN  \n",
       "10333  ('Hoy, a muy tempranas horas, el Papa Francisc...  \n",
       "10835  ('Influenza, finally eradicated ğŸ‘', 'Influenza...  \n",
       "4688   ('ANALISIS DE UNA MUESTRA DE AGUA DEL GRIFO DE...  \n",
       "17560  ('Tomemos en cuenta el Desconocimiento de much...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "posts = posts.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fact_check_id</th>\n",
       "      <th>claim</th>\n",
       "      <th>instances</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131579</th>\n",
       "      <td>155830</td>\n",
       "      <td>('Â« On aide trÃ¨s peu les paysans bio Â»', '\"We ...</td>\n",
       "      <td>[(1645528774.0, 'https://www.mediacites.fr/ver...</td>\n",
       "      <td>('Â« On aide trÃ¨s peu les paysans bio Â»', '\"We ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36821</th>\n",
       "      <td>42162</td>\n",
       "      <td>('Dado que ningÃºn fabricante de las vacunas ap...</td>\n",
       "      <td>[(1639007940.0, 'https://www.newtral.es/autori...</td>\n",
       "      <td>('La autorizaciÃ³n de comercializaciÃ³n de las v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141844</th>\n",
       "      <td>173033</td>\n",
       "      <td>('ÙÙŠØ¯ÙŠÙˆ Ù„Ù„Ù…Ø³Ø¬Ø¯ Ø§Ù„Ù†Ø¨ÙˆÙŠ ÙˆÙ‡Ùˆ Ù…ØºØ·Ù‰ Ø¨Ø§Ù„Ø«Ù„ÙˆØ¬.', \"A v...</td>\n",
       "      <td>[(1579534680.0, 'https://misbar.com/factcheck/...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45892</th>\n",
       "      <td>52785</td>\n",
       "      <td>('En lugar de dejar de usar las aplicaciones d...</td>\n",
       "      <td>[(1658534280.0, 'https://www.univision.com/not...</td>\n",
       "      <td>('Apps de menstruaciÃ³n de Europa: lo que sabem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>3543</td>\n",
       "      <td>('\"MilionÃ¡rio compra clube chinÃªs e exige que ...</td>\n",
       "      <td>[(1622681063.0, 'https://observador.pt/factche...</td>\n",
       "      <td>('Fact Check. MilionÃ¡rio chinÃªs obrigou clube ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        fact_check_id                                              claim  \\\n",
       "131579         155830  ('Â« On aide trÃ¨s peu les paysans bio Â»', '\"We ...   \n",
       "36821           42162  ('Dado que ningÃºn fabricante de las vacunas ap...   \n",
       "141844         173033  ('ÙÙŠØ¯ÙŠÙˆ Ù„Ù„Ù…Ø³Ø¬Ø¯ Ø§Ù„Ù†Ø¨ÙˆÙŠ ÙˆÙ‡Ùˆ Ù…ØºØ·Ù‰ Ø¨Ø§Ù„Ø«Ù„ÙˆØ¬.', \"A v...   \n",
       "45892           52785  ('En lugar de dejar de usar las aplicaciones d...   \n",
       "3430             3543  ('\"MilionÃ¡rio compra clube chinÃªs e exige que ...   \n",
       "\n",
       "                                                instances  \\\n",
       "131579  [(1645528774.0, 'https://www.mediacites.fr/ver...   \n",
       "36821   [(1639007940.0, 'https://www.newtral.es/autori...   \n",
       "141844  [(1579534680.0, 'https://misbar.com/factcheck/...   \n",
       "45892   [(1658534280.0, 'https://www.univision.com/not...   \n",
       "3430    [(1622681063.0, 'https://observador.pt/factche...   \n",
       "\n",
       "                                                    title  \n",
       "131579  ('Â« On aide trÃ¨s peu les paysans bio Â»', '\"We ...  \n",
       "36821   ('La autorizaciÃ³n de comercializaciÃ³n de las v...  \n",
       "141844                                                NaN  \n",
       "45892   ('Apps de menstruaciÃ³n de Europa: lo que sabem...  \n",
       "3430    ('Fact Check. MilionÃ¡rio chinÃªs obrigou clube ...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_checks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_checks['title'].fillna(fact_checks['claim'].apply(lambda x: x[:30] if isinstance(x, str) else \"No Title\"), inplace = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fact_check_id</th>\n",
       "      <th>claim</th>\n",
       "      <th>instances</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131579</th>\n",
       "      <td>155830</td>\n",
       "      <td>('Â« On aide trÃ¨s peu les paysans bio Â»', '\"We ...</td>\n",
       "      <td>[(1645528774.0, 'https://www.mediacites.fr/ver...</td>\n",
       "      <td>('Â« On aide trÃ¨s peu les paysans bio Â»', '\"We ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36821</th>\n",
       "      <td>42162</td>\n",
       "      <td>('Dado que ningÃºn fabricante de las vacunas ap...</td>\n",
       "      <td>[(1639007940.0, 'https://www.newtral.es/autori...</td>\n",
       "      <td>('La autorizaciÃ³n de comercializaciÃ³n de las v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141844</th>\n",
       "      <td>173033</td>\n",
       "      <td>('ÙÙŠØ¯ÙŠÙˆ Ù„Ù„Ù…Ø³Ø¬Ø¯ Ø§Ù„Ù†Ø¨ÙˆÙŠ ÙˆÙ‡Ùˆ Ù…ØºØ·Ù‰ Ø¨Ø§Ù„Ø«Ù„ÙˆØ¬.', \"A v...</td>\n",
       "      <td>[(1579534680.0, 'https://misbar.com/factcheck/...</td>\n",
       "      <td>('ÙÙŠØ¯ÙŠÙˆ Ù„Ù„Ù…Ø³Ø¬Ø¯ Ø§Ù„Ù†Ø¨ÙˆÙŠ ÙˆÙ‡Ùˆ Ù…ØºØ·Ù‰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45892</th>\n",
       "      <td>52785</td>\n",
       "      <td>('En lugar de dejar de usar las aplicaciones d...</td>\n",
       "      <td>[(1658534280.0, 'https://www.univision.com/not...</td>\n",
       "      <td>('Apps de menstruaciÃ³n de Europa: lo que sabem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>3543</td>\n",
       "      <td>('\"MilionÃ¡rio compra clube chinÃªs e exige que ...</td>\n",
       "      <td>[(1622681063.0, 'https://observador.pt/factche...</td>\n",
       "      <td>('Fact Check. MilionÃ¡rio chinÃªs obrigou clube ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        fact_check_id                                              claim  \\\n",
       "131579         155830  ('Â« On aide trÃ¨s peu les paysans bio Â»', '\"We ...   \n",
       "36821           42162  ('Dado que ningÃºn fabricante de las vacunas ap...   \n",
       "141844         173033  ('ÙÙŠØ¯ÙŠÙˆ Ù„Ù„Ù…Ø³Ø¬Ø¯ Ø§Ù„Ù†Ø¨ÙˆÙŠ ÙˆÙ‡Ùˆ Ù…ØºØ·Ù‰ Ø¨Ø§Ù„Ø«Ù„ÙˆØ¬.', \"A v...   \n",
       "45892           52785  ('En lugar de dejar de usar las aplicaciones d...   \n",
       "3430             3543  ('\"MilionÃ¡rio compra clube chinÃªs e exige que ...   \n",
       "\n",
       "                                                instances  \\\n",
       "131579  [(1645528774.0, 'https://www.mediacites.fr/ver...   \n",
       "36821   [(1639007940.0, 'https://www.newtral.es/autori...   \n",
       "141844  [(1579534680.0, 'https://misbar.com/factcheck/...   \n",
       "45892   [(1658534280.0, 'https://www.univision.com/not...   \n",
       "3430    [(1622681063.0, 'https://observador.pt/factche...   \n",
       "\n",
       "                                                    title  \n",
       "131579  ('Â« On aide trÃ¨s peu les paysans bio Â»', '\"We ...  \n",
       "36821   ('La autorizaciÃ³n de comercializaciÃ³n de las v...  \n",
       "141844                     ('ÙÙŠØ¯ÙŠÙˆ Ù„Ù„Ù…Ø³Ø¬Ø¯ Ø§Ù„Ù†Ø¨ÙˆÙŠ ÙˆÙ‡Ùˆ Ù…ØºØ·Ù‰  \n",
       "45892   ('Apps de menstruaciÃ³n de Europa: lo que sabem...  \n",
       "3430    ('Fact Check. MilionÃ¡rio chinÃªs obrigou clube ...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_checks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "post_ids_in_posts = set(posts['post_id'].unique())\n",
    "post_ids_in_pairs = set(pairs['post_id'].unique())\n",
    "\n",
    "\n",
    "missing_post_ids = post_ids_in_pairs - post_ids_in_posts #we are finding pairs that are  in posts.csv but not pairs\n",
    "pairs2 = pairs[~pairs['post_id'].isin(missing_post_ids)]\n",
    "\n",
    "fact_check_ids_in_fact_checks = set(fact_checks['fact_check_id'].unique())\n",
    "fact_check_ids_in_pairs = set(pairs['fact_check_id'].unique())\n",
    "\n",
    "missing_fact_check_ids = fact_check_ids_in_pairs - fact_check_ids_in_fact_checks\n",
    "\n",
    "filtered_pairs = pairs[\n",
    "    ~pairs['post_id'].isin(missing_post_ids) & \n",
    "    ~pairs['fact_check_id'].isin(missing_fact_check_ids)\n",
    "]\n",
    "\n",
    "merged_data = filtered_pairs.merge(posts, on='post_id', how='left')\n",
    "training_data = merged_data.merge(fact_checks, on='fact_check_id', how='left')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Positive Pairs: 27\n",
      "Total Negative Samples: 27\n",
      "{'post_id': 23023, 'fact_check_id': 108741, 'instances_x': \"[(1570019069.0, 'fb')]\", 'ocr': '[(\"Gerard Depardieu #pourquoi la Tunisie? 40 ? Pourquoi la Tunisie Je ne suis pas un politicien et je dÃ©teste les politiciens, car la plupart d\\'entre eux sont des hypocrites corrompus et flatteurs. Encore une fois, ces politiciens interviennent en Tunisie et aident les corrompus comme eux: pourquoi ne laissez-vous pas le peuple tunisien dÃ©cider de son sort? Pour vivre comme les autres peuples du monde, pourquoi insistez-vous pour faire de la Tunisie la gueule de vos dÃ©chets et un foyer de personnes corrompues? Quittez les peuples et quittez leur pays d\\'origine afin que nous ne payions pas vos erreurs dans nos pays d\\'origine. Je vais garder cette fougue odieuse vous jamais.\", \"GÃ©rard Depardieu #why Tunisia? 40 ? Why Tunisia I am not a politician and I hate politicians because most of them they are corrupt and flattering hypocrites. Again, these politicians intervene in Tunisia and help the corrupt like them: why don\\'t you let the Tunisian people decide their fate? For live like the other peoples of the world, why do you insist on make Tunisia the mouth of your waste and a home of people corrupt? Leave the peoples and leave their country of origin so that we don\\'t pay for your mistakes in our home countries. I will keep this hateful passion from you forever.\", [(\\'fra\\', 0.9970784783363342)])]', 'verdicts': \"['False information']\", 'text': '(\\'Ø§Ù„Ù…Ù…Ø«Ù„ Ø§Ù„ÙØ±Ù†Ø³ÙŠ \" Gerard Depardieu\" Ù†Ø·Ù‚ ÙˆØ§Ø¹ØªØ±Ù Ø§Ù„ÙŠ ÙØ±Ù†Ø³Ø§ ØªØªØ¯Ø®Ù„ ÙÙŠ Ø´Ø¤ÙˆÙ† Ø§Ù„Ø´Ø¹ÙˆØ¨ ÙˆØªÙˆÙ†Ø³ ÙˆÙ‡Ø°Ø§ ØªØµØ±ÙŠØ­Ù‡: \"Ù„Ø³Øª Ø³ÙŠØ§Ø³ÙŠØ§ ÙˆØ£ÙƒØ±Ù‡ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠÙŠÙ† Ù„Ø£Ù† Ø£ØºÙ„Ø¨Ù‡Ù… ÙØ§Ø³Ø¯ÙˆÙ† Ù…Ù†Ø§ÙÙ‚ÙˆÙ† Ù…ØªÙ…Ù„Ù‚ÙˆÙ† ØªØ±ÙƒØª ÙØ±Ù†Ø³Ø§ Ù…Ù† Ø£Ø¬Ù„ Ø°Ù„Ùƒ ÙˆÙ‡Ø¬Ø±ØªÙ‡Ø§ Ø­ØªÙ‰ Ù„Ø§ Ø£Ø±Ù‰ ØªÙ„Ùƒ Ø§Ù„ÙˆØ¬ÙˆÙ‡ Ø§Ù„Ù‚Ø¨ÙŠØ­Ø©ØŒ Ø«Ù… Ù…Ø¬Ø¯Ø¯Ø§ Ù‡Ø¤Ù„Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠÙŠÙ† ÙŠØªØ¯Ø®Ù„ÙˆÙ† ÙÙŠ ØªÙˆÙ†Ø³ ÙˆÙŠØ³Ø§Ø¹Ø¯ÙˆÙ† Ø§Ù„ÙØ§Ø³Ø¯ÙŠÙ† Ø£Ù…Ø«Ø§Ù„Ù‡Ù… Ù„Ù…Ø§Ø°Ø§ Ù„Ø§ ØªØªØ±ÙƒÙˆÙ† Ø§Ù„Ø´Ø¹Ø¨ Ø§Ù„ØªÙˆÙ†Ø³ÙŠ ÙŠÙ‚Ø±Ø± Ù…ØµÙŠØ±Ù‡ØŸ Ø­ØªÙ‰ ÙŠØ¹ÙŠØ´ ÙƒØ¨Ø§Ù‚ÙŠ Ø´Ø¹ÙˆØ¨ Ø§Ù„Ø¹Ø§Ù„Ù… Ù„Ù…Ø§Ø°Ø§ ØªØµØ±ÙˆÙ† Ø¹Ù„Ù‰ Ø¬Ø¹Ù„ ØªÙˆÙ†Ø³ Ù…ØµØ¨Ø§ Ù„ÙØ¶Ù„Ø§ØªÙƒÙ… ÙˆÙ…Ø±ØªØ¹Ø§ Ù„Ù„ÙØ§Ø³Ø¯ÙŠÙ†ØŸØŸ Ø§ØªØ±ÙƒÙˆØ§ Ø§Ù„Ø´Ø¹ÙˆØ¨ ÙˆØ§Ø±Ø­Ù„ÙˆØ§ Ù…Ù† Ø£ÙˆØ·Ø§Ù†Ù‡Ù… Ø­ØªÙ‰ Ù„Ø§ Ù†Ø¯ÙØ¹ Ø«Ù…Ù† Ø£Ø®Ø·Ø§Ø¦ÙƒÙ… ÙÙŠ Ø£ÙˆØ·Ø§Ù†Ù†Ø§.. Ø³Ø£Ø¨Ù‚Ù‰ Ø°Ù„Ùƒ Ø§Ù„Ù…Ø´Ø§ÙƒØ³ Ø§Ù„Ø¨ØºÙŠØ¶ Ù„ÙƒÙ… Ø£Ø¨Ø¯Ø§.\"\\', \\'The French actor \"Gerard Depardieu\" spoke and admitted to France interfering in the affairs of peoples and Tunisia, and this is his statement: \"I am not a politician, and I hate politicians because most of them are corrupt, hypocritical, and sycophants. I left France for this and emigrated so that I would not see those ugly faces. Then again, these politicians interfere in Tunisia and help corrupt people like them. Why don\\\\\\'t you let the Tunisian people decide their fate? So that they live like the rest of the world\\\\\\'s peoples. Why do you insist on making Tunisia is a dumping ground for your waste and a hotbed for the corrupt?? Leave the peoples and leave their countries so that we do not pay the price for your mistakes in our countries.\\', [(\\'ara\\', 1.0)])', 'claim': '(\"President Biden said the US didn\\'t have a Covid Vaccine until he took office\", \"President Biden said the US didn\\'t have a Covid Vaccine until he took office\", [(\\'eng\\', 1.0)])', 'instances_y': \"[(1613761873.0, 'https://leadstories.com/hoax-alert/2021/02/biden-did-not-say-us-lacked-covid-vaccine-before-he-took-office.html#d4b2f553782bb023360f52db10def12f')]\", 'title': \"('Fact Check: President Biden Quote About Not Having Covid-19 Vaccine Coming Into Office Was About Supply Backlog -- NOT About Vaccine Existence', 'Fact Check: President Biden Quote About Not Having Covid-19 Vaccine Coming Into Office Was About Supply Backlog -- NOT About Vaccine Existence', [('eng', 1.0)])\", 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Create positive pairs list\n",
    "pos_pairs = []\n",
    "for _, row in training_data.iterrows():\n",
    "    pair_dict = row.to_dict()\n",
    "    pair_dict['label'] = 1  # Positive label\n",
    "    pos_pairs.append(pair_dict)\n",
    "\n",
    "print(f\"Total Positive Pairs: {len(pos_pairs)}\")\n",
    "\n",
    "\n",
    "fact_check_dict = fact_checks.set_index('fact_check_id').to_dict('index') \n",
    "\n",
    "\n",
    "neg_pairs = []\n",
    "for _, row in training_data.iterrows():\n",
    "    post_text = row['text']\n",
    "    correct_fact_check_id = row['fact_check_id']\n",
    "\n",
    "    while True:\n",
    "        random_fact_id = random.choice(list(fact_check_dict.keys()))\n",
    "        if random_fact_id != correct_fact_check_id:\n",
    "            break\n",
    "\n",
    "    random_fact_check_data = fact_check_dict[random_fact_id]\n",
    "\n",
    "  \n",
    "    if pd.notna(post_text) and pd.notna(random_fact_check_data['claim']):\n",
    "        neg_pairs.append({\n",
    "            'post_id': row['post_id'],  \n",
    "            'fact_check_id': random_fact_id,  \n",
    "            'instances_x': row['instances_x'],  \n",
    "            'ocr': row['ocr'], \n",
    "            'verdicts': row['verdicts'], \n",
    "            'text': post_text,  \n",
    "            'claim': random_fact_check_data['claim'], \n",
    "            'instances_y': random_fact_check_data['instances'],\n",
    "            'title': random_fact_check_data['title'],  \n",
    "            'label': 0  \n",
    "        })\n",
    "\n",
    "print(f\"Total Negative Samples: {len(neg_pairs)}\")\n",
    "print(neg_pairs[0]) \n",
    "all_pairs = pos_pairs + neg_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_pairs)\n",
    "df = df.drop(columns = ['post_id', 'fact_check_id', 'instances_x','instances_y' ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ocr</th>\n",
       "      <th>verdicts</th>\n",
       "      <th>text</th>\n",
       "      <th>claim</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(\"Gerard Depardieu #pourquoi la Tunisie? 40 ?...</td>\n",
       "      <td>['False information']</td>\n",
       "      <td>('Ø§Ù„Ù…Ù…Ø«Ù„ Ø§Ù„ÙØ±Ù†Ø³ÙŠ \" Gerard Depardieu\" Ù†Ø·Ù‚ ÙˆØ§Ø¹ØªØ±...</td>\n",
       "      <td>('Ø§Ù„Ù…Ù…Ø«Ù„ Ø§Ù„ÙØ±Ù†Ø³ÙŠ Gerard Depardieu Ù†Ø·Ù‚ ÙˆØ§Ø¹ØªØ±Ù Ø£...</td>\n",
       "      <td>('Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø© Ù„Ù„Ù…Ù…Ø«Ù‘Ù„ Ø¬ÙŠØ±Ø§Ø± Ø¯ÙˆØ¨Ø§Ø±Ø¯ÙŠÙˆ Ù…Ø±ÙƒÙ‘Ø¨Ø©', '...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('MARVIN BERGAUER REPORTER OE24TV WIEN: DEMO ...</td>\n",
       "      <td>['Missing context']</td>\n",
       "      <td>('ğŸ˜‚ Milagros del cambio plandÃ©mico... ğŸ‘‡', 'ğŸ˜‚ M...</td>\n",
       "      <td>('VÃ­deo de un informativo en el que un cadÃ¡ver...</td>\n",
       "      <td>('No, este vÃ­deo de un informativo austriaco e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>('GRAN FRASE ğŸ¤º â€œQuerido Sancho: Compruebo con ...</td>\n",
       "      <td>('Sabio Don Miguel de Cervantes. â€œQuerido Sanc...</td>\n",
       "      <td>(\"Querido Sancho compruebo con pesar...: la ci...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>['Missing context.']</td>\n",
       "      <td>('Tarian klasik ini baru diciptakan di China d...</td>\n",
       "      <td>('Video penari robot buatan Tiongkok tampil di...</td>\n",
       "      <td>(\"Penari di video ini adalah manusia, bukan 'r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>['False information']</td>\n",
       "      <td>('à¤œà¤¾à¤«à¤¼à¤°à¤¾à¤¬à¤¾à¤¦ à¤¨à¤ˆ à¤¦à¤¿à¤²à¥à¤²à¥€ à¤•à¥‡ à¤¦à¥ƒà¤¶à¥à¤¯.. à¤²à¤—à¤¤à¤¾ à¤¹à¥ˆ à¤ªà¥à¤°à¤§à¤¾...</td>\n",
       "      <td>(\"Video shows COVID protocol flouted in Delhi'...</td>\n",
       "      <td>('Video From Lahore Passed Off as COVID-19 Nor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ocr               verdicts  \\\n",
       "0  [(\"Gerard Depardieu #pourquoi la Tunisie? 40 ?...  ['False information']   \n",
       "1  [('MARVIN BERGAUER REPORTER OE24TV WIEN: DEMO ...    ['Missing context']   \n",
       "2                                                 []                     []   \n",
       "3                                                 []   ['Missing context.']   \n",
       "4                                                 []  ['False information']   \n",
       "\n",
       "                                                text  \\\n",
       "0  ('Ø§Ù„Ù…Ù…Ø«Ù„ Ø§Ù„ÙØ±Ù†Ø³ÙŠ \" Gerard Depardieu\" Ù†Ø·Ù‚ ÙˆØ§Ø¹ØªØ±...   \n",
       "1  ('ğŸ˜‚ Milagros del cambio plandÃ©mico... ğŸ‘‡', 'ğŸ˜‚ M...   \n",
       "2  ('GRAN FRASE ğŸ¤º â€œQuerido Sancho: Compruebo con ...   \n",
       "3  ('Tarian klasik ini baru diciptakan di China d...   \n",
       "4  ('à¤œà¤¾à¤«à¤¼à¤°à¤¾à¤¬à¤¾à¤¦ à¤¨à¤ˆ à¤¦à¤¿à¤²à¥à¤²à¥€ à¤•à¥‡ à¤¦à¥ƒà¤¶à¥à¤¯.. à¤²à¤—à¤¤à¤¾ à¤¹à¥ˆ à¤ªà¥à¤°à¤§à¤¾...   \n",
       "\n",
       "                                               claim  \\\n",
       "0  ('Ø§Ù„Ù…Ù…Ø«Ù„ Ø§Ù„ÙØ±Ù†Ø³ÙŠ Gerard Depardieu Ù†Ø·Ù‚ ÙˆØ§Ø¹ØªØ±Ù Ø£...   \n",
       "1  ('VÃ­deo de un informativo en el que un cadÃ¡ver...   \n",
       "2  ('Sabio Don Miguel de Cervantes. â€œQuerido Sanc...   \n",
       "3  ('Video penari robot buatan Tiongkok tampil di...   \n",
       "4  (\"Video shows COVID protocol flouted in Delhi'...   \n",
       "\n",
       "                                               title  label  \n",
       "0  ('Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø© Ù„Ù„Ù…Ù…Ø«Ù‘Ù„ Ø¬ÙŠØ±Ø§Ø± Ø¯ÙˆØ¨Ø§Ø±Ø¯ÙŠÙˆ Ù…Ø±ÙƒÙ‘Ø¨Ø©', '...      1  \n",
       "1  ('No, este vÃ­deo de un informativo austriaco e...      1  \n",
       "2  (\"Querido Sancho compruebo con pesar...: la ci...      1  \n",
       "3  (\"Penari di video ini adalah manusia, bukan 'r...      1  \n",
       "4  ('Video From Lahore Passed Off as COVID-19 Nor...      1  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, EncoderDecoderModel, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "\n",
    "def preprocess_data(ocr, verdicts, text):\n",
    "    combined_input = ocr + \" [SEP] \" + verdicts + \" [SEP] \" + text\n",
    "    return combined_input\n",
    "\n",
    "# Concatenate relevant information for both train and test\n",
    "train_inputs = [preprocess_data(a, b, c) for a, b, c in zip(train_df['ocr'], train_df['verdicts'], train_df['text'])]\n",
    "test_inputs = [preprocess_data(a, b, c) for a, b, c in zip(test_df['ocr'], test_df['verdicts'], test_df['text'])]\n",
    "\n",
    "# Tokenize input text (input sentences) and output text (claims)\n",
    "train_encodings = tokenizer(train_inputs, padding=\"max_length\", truncation=True, max_length=128, return_tensors='pt')\n",
    "test_encodings = tokenizer(test_inputs, padding=\"max_length\", truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Tokenize the claims (outputs)\n",
    "train_claims_encodings = tokenizer(train_df['claim'].tolist(), padding=\"max_length\", truncation=True, max_length=128, return_tensors='pt')\n",
    "test_claims_encodings = tokenizer(test_df['claim'].tolist(), padding=\"max_length\", truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels['input_ids'][idx]  # Use the input_ids of the target (claim) as labels\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels['input_ids'])\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, train_claims_encodings)\n",
    "test_dataset = CustomDataset(test_encodings, test_claims_encodings)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/muqaddasharoon/opt/anaconda3/envs/lab/lib/python3.11/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:623: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/Users/muqaddasharoon/opt/anaconda3/envs/lab/lib/python3.11/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 11.256892999013266\n",
      "Epoch 2/3, Loss: 6.579164028167725\n",
      "Epoch 3/3, Loss: 4.618646105130513\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, EncoderDecoderModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-multilingual-cased', 'bert-base-multilingual-cased')\n",
    "\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_train_loss}\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Implement your evaluation loop here if needed\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
