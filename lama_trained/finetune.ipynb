{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/haroonm0/localdisk/.conda/envs/.conda_envs_dir_test/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from lama_train import *\n",
    "from lama_utils import *\n",
    "from real_work.pre_process.utilities import *\n",
    "from real_work.pre_process.pre_process_orig import *\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    max_seq_length = 2048\n",
    "    dtype = None\n",
    "    load_in_4bit = True\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"/home/stud/haroonm0/localdisk/Fact_check/lora_model5\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        device_map=\"auto\",  # Automatically map layers to GPU/CPU  # Enable CPU offload\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_native_types(data):\n",
    "    if isinstance(data, dict):\n",
    "        return {str(key): convert_to_native_types(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_to_native_types(item) for item in data]\n",
    "    elif isinstance(data, (np.int64, np.int32)):\n",
    "        return int(data)\n",
    "    elif isinstance(data, (np.float64, np.float32)):\n",
    "        return float(data)\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "def replace_short_text(row):\n",
    "    text_type = 'text_original' \n",
    "    claim_type = 'original_claim'\n",
    "    ocr_type = 'ocr_original'\n",
    "    if isinstance(row[text_type], str) and len(row[text_type].split()) < 5:\n",
    "            return row[ocr_type] if isinstance(row[ocr_type], str) else row[text_type]\n",
    "    return row[text_type]\n",
    "\n",
    "def start():\n",
    "    text_type = 'text_original' \n",
    "    claim_type = 'original_claim'\n",
    "    ocr_type = 'ocr_original'\n",
    "    \n",
    "    with open('/home/stud/haroonm0/localdisk/Fact_check/dataset/crosslingual_predictions1.json', 'r') as f:\n",
    "        crosslingual_predictions = json.load(f)\n",
    "\n",
    "    \n",
    "    \n",
    "    fact_checks = pd.read_csv('/home/stud/haroonm0/localdisk/Fact_check/dataset/fact_checks1.csv')\n",
    "    fact_checks = fact_checks.sample(frac=0.1)\n",
    "    posts = pd.read_csv('/home/stud/haroonm0/localdisk/Fact_check/dataset/posts1.csv')\n",
    "    #posts = posts.sample(frac=0.01)\n",
    "    \n",
    "    \n",
    "    post_ids = list(map(int, crosslingual_predictions.keys()))\n",
    "    \n",
    "    matched_posts = posts[posts['post_id'].isin(post_ids)]\n",
    "\n",
    "\n",
    "    matched_posts, fact_checks = pre_process_test(matched_posts, fact_checks, text_type)\n",
    "    matched_posts.loc[:, text_type] = matched_posts.apply(replace_short_text, axis=1)\n",
    "    \n",
    "    post_texts = matched_posts.set_index('post_id')[text_type]\n",
    "    #post_texts = post_texts[:2]\n",
    "    claims = [{\"original_claim\": row[claim_type], \"fact_check_id\": row[\"fact_check_id\"]} for _, row in fact_checks.iterrows()]\n",
    "    \n",
    "\n",
    "    print(\"started embeddings\")\n",
    "    #using normal sentence embedder\n",
    "    simple = False\n",
    "    if simple:\n",
    "        post_embeddings, claim_embeddings = embed_posts_and_claims(post_texts.tolist(), [claim[claim_type] for claim in claims])\n",
    "        print(\"started retriveing top 50\")\n",
    "        top_50_claims_per_post = retrieve_top_claims(post_embeddings, claim_embeddings, claims, top_k=50)\n",
    "    else:\n",
    "        top_50_claims_per_post = docArray(post_texts, claims)\n",
    "        print(\"top 50 claims are\")\n",
    "        print(top_50_claims_per_post)\n",
    "        \n",
    "    return top_50_claims_per_post, post_ids, posts, claims, post_texts \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    crosslingual_predictions = {}\n",
    "    \n",
    "    csv_data = []\n",
    "\n",
    "    for post_entry in top_50_claims_per_post:\n",
    "        post_id = post_ids[post_entry[\"post_id\"]]\n",
    "        post_text = post_texts.loc[post_id]\n",
    "        top_claims = post_entry[\"top_claims\"]\n",
    "\n",
    "        ranked_claims = rank_claims(post_text, top_claims, model, tokenizer)\n",
    "        ranked_fact_check_ids = [claim[\"claim_text\"][\"fact_check_id\"] for claim in ranked_claims]\n",
    "        crosslingual_predictions[str(post_id)] = ranked_fact_check_ids\n",
    "\n",
    "        for rank, claim in enumerate(ranked_claims, start=1):\n",
    "            csv_data.append({\n",
    "                \"post_id\": post_id,\n",
    "                \"post_text\": post_text,\n",
    "                \"rank\": rank,\n",
    "                \"fact_check_id\": claim[\"claim_text\"][\"fact_check_id\"],\n",
    "                \"claim_text\": claim[\"claim_text\"][claim_type],\n",
    "                \"score\": claim[\"score\"]\n",
    "            })\n",
    "\n",
    "    crosslingual_predictions_native = convert_to_native_types(crosslingual_predictions)\n",
    "\n",
    "    csv_file = '/home/stud/haroonm0/localdisk/Fact_check/mistraal_finetune/retrieved_claims.csv'\n",
    "    csv_df = pd.DataFrame(csv_data)\n",
    "    csv_df.to_csv(csv_file, index=False)\n",
    "    print(f\"Retrieved claims saved to CSV at {csv_file}\")\n",
    "\n",
    "    output_file = '/home/stud/haroonm0/localdisk/Fact_check/mistraal_finetune/crosslingual_predictions5.json'\n",
    "    try:\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(crosslingual_predictions_native, f, indent=4)\n",
    "        print(f\"Updated crosslingual_predictions saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error saving JSON:\", e) \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_claims_per_post, post_ids, posts, claims, post_texts = start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpaca_prompt = \"\"\"\n",
    "  \n",
    " ### Instruction:\n",
    "Your task is to determine if the following claim is supported by the given post.\n",
    "\n",
    "### Definitions:\n",
    "- **Claim**: A claim is a statement that asserts something to be true, factual, or believable. It may be a fact, an opinion, or a prediction that can be verified or debated. Claims often require evidence or reasoning to determine their validity.\n",
    "- **Post**: A post is a piece of text, often from social media, that is user-generated and may or may not be verified. Posts can include opinions, observations, rumors, or misinformation.\n",
    "\n",
    "### Task:\n",
    "- Compare the claim with the information provided in the post.\n",
    "- Determine whether the claim is:\n",
    "    1. **Supported**: The post provides sufficient evidence or information to confirm the claim.\n",
    "    2. **Refuted**: The post provides sufficient evidence or information to contradict the claim.\n",
    "    3. **Not Enough Information**: The post does not provide sufficient evidence to determine whether the claim is true or false.\n",
    "- If the post contains contradictory or ambiguous information, explain why the claim cannot be verified.\n",
    "\n",
    "### Guidelines:\n",
    "- Carefully evaluate the relationship between the post and the claim.\n",
    "- Avoid making assumptions beyond the content of the post.\n",
    "- Focus on the specific evidence provided in the post.\n",
    "- Provide a clear and concise response: either **Supported**, **Refuted**, or **Not Enough Information**.\n",
    "\n",
    "### Post:\n",
    "{}\n",
    "\n",
    "### Claim:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}.  \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def rank_claims_docArray(post, claims, model, tokenizer, max_length=1024):\n",
    "    \"\"\"\n",
    "    Ranks claims for a given post by semantic similarity using embeddings.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    for claim in claims:\n",
    "        # Create input using the Alpaca-style prompt\n",
    "        input_text = alpaca_prompt.format(post, claim[\"original_claim\"], \"\")\n",
    "        \n",
    "        # Tokenize the input\n",
    "        tokenized_input = tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        # Model inference with hidden states\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokenized_input, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[-1]  # Last hidden layer\n",
    "\n",
    "        # Compute embeddings (mean pooling)\n",
    "        embedding = hidden_states.mean(dim=1)  # Shape: [batch_size, hidden_size]\n",
    "\n",
    "        # Compute similarity (assuming single batch item)\n",
    "        score = cosine_similarity(embedding[0], embedding[0], dim=0).item()\n",
    "        scores.append(score)\n",
    "\n",
    "    # Rank claims by scores\n",
    "    ranked_indices = sorted(range(len(scores)), key=lambda idx: scores[idx], reverse=True)\n",
    "    ranked_claims = [\n",
    "        {\n",
    "            \"original_claim\": claims[idx][\"original_claim\"],\n",
    "            \"fact_check_id\": claims[idx][\"fact_check_id\"],\n",
    "            \"score\": scores[idx]\n",
    "        }\n",
    "        for idx in ranked_indices\n",
    "    ]\n",
    "\n",
    "    return ranked_claims[:50]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started with lama\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m top_claims \u001b[38;5;241m=\u001b[39m post_entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_claims\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#print(f\"len of claims is {len(top_claims)}\")\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m ranked_claims \u001b[38;5;241m=\u001b[39m rank_claims_docArray(post_text, top_claims, \u001b[43mmodel\u001b[49m, tokenizer)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#print(\"ranked claims are...\")\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#print(ranked_claims)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m ranked_fact_check_ids \u001b[38;5;241m=\u001b[39m [claim[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfact_check_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m claim \u001b[38;5;129;01min\u001b[39;00m ranked_claims]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"started with lama\")\n",
    "\n",
    "crosslingual_predictions = {}\n",
    "csv_data = []\n",
    "\n",
    "for post_entry in top_50_claims_per_post:\n",
    "    post_id = post_ids[post_entry[\"post_id\"]]\n",
    "    if post_id not in post_texts.index:\n",
    "        print(f\"Warning: post_id {post_id} not found in post_texts index. Skipping.\")\n",
    "        continue\n",
    "    post_text = post_texts.loc[post_id]\n",
    "    top_claims = post_entry[\"top_claims\"]\n",
    "    #print(f\"len of claims is {len(top_claims)}\")\n",
    "    ranked_claims = rank_claims_docArray(post_text, top_claims, model, tokenizer)\n",
    "    #print(\"ranked claims are...\")\n",
    "    #print(ranked_claims)\n",
    "\n",
    "    ranked_fact_check_ids = [claim[\"fact_check_id\"] for claim in ranked_claims]\n",
    "    crosslingual_predictions[str(post_id)] = ranked_fact_check_ids\n",
    "\n",
    "    for rank, claim in enumerate(ranked_claims, start=1):\n",
    "        csv_data.append({\n",
    "            \"post_id\": post_id,\n",
    "            \"post_text\": post_text,\n",
    "            \"rank\": rank,\n",
    "            \"fact_check_id\": claim[\"fact_check_id\"],\n",
    "            \"claim_text\": claim[\"original_claim\"],\n",
    "            \"score\": float(claim[\"score\"])\n",
    "        })\n",
    "\n",
    "crosslingual_predictions_native = convert_to_native_types(crosslingual_predictions)\n",
    "\n",
    "output_file = 'crosslingual_predictions5.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(crosslingual_predictions_native, f, indent=4)\n",
    "print(f\"Updated crosslingual_predictions saved to {output_file}\")\n",
    "\n",
    "csv_file = 'retrieved_claims.csv'\n",
    "csv_df = pd.DataFrame(csv_data)\n",
    "csv_df.to_csv(csv_file, index=False)\n",
    "print(f\"Retrieved claims saved to CSV at {csv_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
