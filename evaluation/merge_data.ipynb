{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import ast\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_checks = pd.read_csv(\"../dataset/fact_checks.csv\")\n",
    "posts = pd.read_csv(\"../dataset/posts.csv\")\n",
    "pairs = pd.read_csv(\"../dataset/pairs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_text_with_ocr(df, text_column='text', ocr_column='ocr'):\n",
    "    df[text_column] = df.apply(\n",
    "        lambda row: row[ocr_column] if pd.isna(row[text_column]) or row[text_column].strip() == '' else row[text_column],\n",
    "        axis=1\n",
    "    )\n",
    "    return df\n",
    "import ast\n",
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "def safe_literal_eval(val):\n",
    "    try:\n",
    "        if isinstance(val, str):\n",
    "            parsed_val = ast.literal_eval(val)\n",
    "            if isinstance(parsed_val, list):\n",
    "                for item in parsed_val:\n",
    "                    if isinstance(item, tuple) and len(item) == 3:\n",
    "                        return item  # Return the first valid tuple\n",
    "            elif isinstance(parsed_val, tuple) and len(parsed_val) == 3:\n",
    "                return parsed_val\n",
    "        return val\n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        print(f\"Literal eval failed for {val}: {e}\")\n",
    "        return (None, None, None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_text_column(row, row_name):\n",
    "    try:\n",
    "        parsed = safe_literal_eval(row[row_name])\n",
    "        if isinstance(parsed, tuple) and len(parsed) == 3:\n",
    "            first_text = parsed[0].strip() if isinstance(parsed[0], str) else None\n",
    "            second_text = parsed[1].strip() if isinstance(parsed[1], str) else None\n",
    "            \n",
    "            # Extract the language with the highest confidence\n",
    "            if isinstance(parsed[2], list) and all(isinstance(item, tuple) for item in parsed[2]):\n",
    "                lang_conf = max(parsed[2], key=lambda x: x[1] if len(x) == 2 else 0)\n",
    "            else:\n",
    "                lang_conf = (None, None)\n",
    "            \n",
    "            lang = lang_conf[0]\n",
    "            confidence = lang_conf[1]\n",
    "            return pd.Series([first_text, second_text, lang, confidence])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {row[row_name]} -> {e}\")\n",
    "    return pd.Series([None, None, None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = replace_text_with_ocr(posts, text_column='text', ocr_column='ocr')\n",
    "posts[['ocr_original', 'ocr_translated', 'ocr_language', 'ocr_confidence']] = posts.apply(\n",
    "    lambda row: split_text_column(row, 'ocr'), axis=1\n",
    ")\n",
    "\n",
    "posts[['text_original', 'text_translated', 'text_language', 'text_confidence']] = posts.apply(\n",
    "    lambda row: split_text_column(row, 'text'), axis=1\n",
    ")\n",
    "\n",
    "\n",
    "fact_checks[['original_claim', 'translated_claim', 'language', 'confidence']] = fact_checks.apply(\n",
    "    lambda row: split_text_column(row, 'claim'), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pos_neg_pairs(train_df, fact_checks):\n",
    "    pos_pairs = []\n",
    "    for _, row in train_df.iterrows():\n",
    "        pair_dict = row.to_dict()\n",
    "        pair_dict['label'] = 1  # Positive label\n",
    "        pos_pairs.append(pair_dict)\n",
    "    \n",
    "    print(f\"Total Positive Pairs: {len(pos_pairs)}\")\n",
    "\n",
    "    fact_check_dict = fact_checks.set_index('fact_check_id').to_dict('index')\n",
    "\n",
    "    neg_pairs = []\n",
    "    for _, row in train_df.iterrows():\n",
    "        post_text = row['text']\n",
    "        correct_fact_check_id = row['fact_check_id']\n",
    "\n",
    "        while True:\n",
    "            random_fact_id = random.choice(list(fact_check_dict.keys()))\n",
    "            if random_fact_id != correct_fact_check_id:\n",
    "                break\n",
    "\n",
    "        random_fact_check_data = fact_check_dict[random_fact_id]\n",
    "\n",
    "        if pd.notna(post_text) and pd.notna(random_fact_check_data['claim']):\n",
    "            neg_pairs.append({ \n",
    "                'ocr_original': row['ocr_original'], \n",
    "                'ocr_translated': row['ocr_translated'], \n",
    "                'text_original': row['text_original'], \n",
    "                'text_translated': row['text_translated'],\n",
    "                'original_claim': random_fact_check_data['original_claim'], \n",
    "                'translated_claim': random_fact_check_data['translated_claim'],\n",
    "                'label': 0  \n",
    "            })\n",
    "\n",
    "    print(f\"Total Negative Samples: {len(neg_pairs)}\")\n",
    "    all_pairs = pos_pairs + neg_pairs\n",
    "    df = pd.DataFrame(all_pairs)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_data(posts, fact_checks, pairs):\n",
    "\n",
    "    posts = posts.drop_duplicates(subset='post_id')\n",
    "    fact_checks = fact_checks.drop_duplicates(subset='fact_check_id')\n",
    "    \n",
    "    merged_data = pairs.merge(posts, on='post_id', how='left').merge(fact_checks, on='fact_check_id', how='left')\n",
    "    \n",
    "    merged_data.drop(columns=['instances_x', 'verdicts', 'ocr_confidence', 'instances_y', 'confidence'], inplace=True)\n",
    "    \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   post_id                                                ocr  \\\n",
      "0     2228  [('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...   \n",
      "1     2228  [('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...   \n",
      "2     2228  [('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...   \n",
      "3     2229  [('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...   \n",
      "4     2229  [('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...   \n",
      "5     2229  [('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...   \n",
      "6     6088  [('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...   \n",
      "7     6088  [('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...   \n",
      "8     6088  [('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...   \n",
      "9    21033                                                 []   \n",
      "\n",
      "                                                text  \n",
      "0  [('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...  \n",
      "1  [('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...  \n",
      "2  [('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...  \n",
      "3  [('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...  \n",
      "4  [('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...  \n",
      "5  [('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...  \n",
      "6  ('Asking for a friend...', 'Asking for a frien...  \n",
      "7  ('Asking for a friend...', 'Asking for a frien...  \n",
      "8  ('Asking for a friend...', 'Asking for a frien...  \n",
      "9  ('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHEN...  \n",
      "Total Positive Pairs: 20594\n",
      "Total Negative Samples: 20594\n",
      "Total Positive Pairs: 2575\n",
      "Total Negative Samples: 2575\n",
      "Total Positive Pairs: 2574\n",
      "Total Negative Samples: 2574\n"
     ]
    }
   ],
   "source": [
    "merged_data = merge_data(posts, fact_checks, pairs)\n",
    "\n",
    "\n",
    "train_df, temp_df = train_test_split(merged_data, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_pairs = create_pos_neg_pairs(train_df, fact_checks)\n",
    "test_pairs = create_pos_neg_pairs(test_df, fact_checks)\n",
    "val_pairs = create_pos_neg_pairs(val_df, fact_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>fact_check_id</th>\n",
       "      <th>ocr</th>\n",
       "      <th>text</th>\n",
       "      <th>ocr_original</th>\n",
       "      <th>ocr_translated</th>\n",
       "      <th>ocr_language</th>\n",
       "      <th>text_original</th>\n",
       "      <th>text_translated</th>\n",
       "      <th>text_language</th>\n",
       "      <th>text_confidence</th>\n",
       "      <th>claim</th>\n",
       "      <th>title</th>\n",
       "      <th>original_claim</th>\n",
       "      <th>translated_claim</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2228</td>\n",
       "      <td>33</td>\n",
       "      <td>[('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...</td>\n",
       "      <td>[('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...</td>\n",
       "      <td>WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHEN A...</td>\n",
       "      <td>WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHEN A...</td>\n",
       "      <td>eng</td>\n",
       "      <td>WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHEN A...</td>\n",
       "      <td>WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHEN A...</td>\n",
       "      <td>eng</td>\n",
       "      <td>1.0</td>\n",
       "      <td>('\"$4 trillion jobs plan\" unnecessary because ...</td>\n",
       "      <td>('Posts on Biden jobs plan falsely claim 2020 ...</td>\n",
       "      <td>\"$4 trillion jobs plan\" unnecessary because 20...</td>\n",
       "      <td>\"$4 trillion jobs plan\" unnecessary because 20...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2228</td>\n",
       "      <td>23568</td>\n",
       "      <td>[('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...</td>\n",
       "      <td>[('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...</td>\n",
       "      <td>WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHEN A...</td>\n",
       "      <td>WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHEN A...</td>\n",
       "      <td>eng</td>\n",
       "      <td>WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHEN A...</td>\n",
       "      <td>WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHEN A...</td>\n",
       "      <td>eng</td>\n",
       "      <td>1.0</td>\n",
       "      <td>('America had the lowest unemployment rate in ...</td>\n",
       "      <td>('Fact check: Unemployment rate hit historic h...</td>\n",
       "      <td>America had the lowest unemployment rate in hi...</td>\n",
       "      <td>America had the lowest unemployment rate in hi...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  fact_check_id                                                ocr  \\\n",
       "0     2228             33  [('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...   \n",
       "1     2228          23568  [('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...   \n",
       "\n",
       "                                                text  \\\n",
       "0  [('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...   \n",
       "1  [('WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHE...   \n",
       "\n",
       "                                        ocr_original  \\\n",
       "0  WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHEN A...   \n",
       "1  WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHEN A...   \n",
       "\n",
       "                                      ocr_translated ocr_language  \\\n",
       "0  WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHEN A...          eng   \n",
       "1  WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHEN A...          eng   \n",
       "\n",
       "                                       text_original  \\\n",
       "0  WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHEN A...   \n",
       "1  WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHEN A...   \n",
       "\n",
       "                                     text_translated text_language  \\\n",
       "0  WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHEN A...           eng   \n",
       "1  WHY DO WE NEED A $4 TRILLION JOBS PLAN, WHEN A...           eng   \n",
       "\n",
       "   text_confidence                                              claim  \\\n",
       "0              1.0  ('\"$4 trillion jobs plan\" unnecessary because ...   \n",
       "1              1.0  ('America had the lowest unemployment rate in ...   \n",
       "\n",
       "                                               title  \\\n",
       "0  ('Posts on Biden jobs plan falsely claim 2020 ...   \n",
       "1  ('Fact check: Unemployment rate hit historic h...   \n",
       "\n",
       "                                      original_claim  \\\n",
       "0  \"$4 trillion jobs plan\" unnecessary because 20...   \n",
       "1  America had the lowest unemployment rate in hi...   \n",
       "\n",
       "                                    translated_claim language  \n",
       "0  \"$4 trillion jobs plan\" unnecessary because 20...      eng  \n",
       "1  America had the lowest unemployment rate in hi...      eng  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>fact_check_id</th>\n",
       "      <th>ocr</th>\n",
       "      <th>text</th>\n",
       "      <th>ocr_original</th>\n",
       "      <th>ocr_translated</th>\n",
       "      <th>ocr_language</th>\n",
       "      <th>text_original</th>\n",
       "      <th>text_translated</th>\n",
       "      <th>text_language</th>\n",
       "      <th>text_confidence</th>\n",
       "      <th>claim</th>\n",
       "      <th>title</th>\n",
       "      <th>original_claim</th>\n",
       "      <th>translated_claim</th>\n",
       "      <th>language</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10719.0</td>\n",
       "      <td>84382.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>('Fue en 1908. Los belgas leyendo la Biblia an...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Fue en 1908. Los belgas leyendo la Biblia ante...</td>\n",
       "      <td>It was in 1908. Belgians reading the Bible bef...</td>\n",
       "      <td>spa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>('Los belgas leyendo la Biblia antes de colgar...</td>\n",
       "      <td>('Según historiadores, es improbable que la fo...</td>\n",
       "      <td>Los belgas leyendo la Biblia antes de colgar a...</td>\n",
       "      <td>Belgians reading the Bible before hanging a 7-...</td>\n",
       "      <td>spa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21478.0</td>\n",
       "      <td>56588.0</td>\n",
       "      <td>[('TN Todo Noticias [USER] Noticias ULTIMO MOM...</td>\n",
       "      <td>('Y seguimos sumando muertes... [URL]', 'And w...</td>\n",
       "      <td>TN Todo Noticias [USER] Noticias ULTIMO MOMENT...</td>\n",
       "      <td>TN All News [USER] News LAST MOMENT The journa...</td>\n",
       "      <td>spa</td>\n",
       "      <td>Y seguimos sumando muertes... [URL]</td>\n",
       "      <td>And we keep adding deaths... [URL]</td>\n",
       "      <td>spa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>('Falleció el periodista Carlos Ferrara, que h...</td>\n",
       "      <td>('El periodista argentino Carlos Ferrara no mu...</td>\n",
       "      <td>Falleció el periodista Carlos Ferrara, que hab...</td>\n",
       "      <td>Journalist Carlos Ferrara died, who had fainte...</td>\n",
       "      <td>spa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14336.0</td>\n",
       "      <td>103924.0</td>\n",
       "      <td>[('ان شهادت ه امام من شریک تمبوتراب اسکاوٹ', '...</td>\n",
       "      <td>('Masjid to carona ha lakin yha nae ha carona ...</td>\n",
       "      <td>ان شهادت ه امام من شریک تمبوتراب اسکاوٹ</td>\n",
       "      <td>that testimony My Imam Tembutrab Scout partner</td>\n",
       "      <td>urd</td>\n",
       "      <td>Masjid to carona ha lakin yha nae ha carona ku...</td>\n",
       "      <td>Masjid to carona ha lakin yha nae ha carona ku...</td>\n",
       "      <td>hin</td>\n",
       "      <td>1.0</td>\n",
       "      <td>('Photo shows Syed Murad Ali Shah, chief minis...</td>\n",
       "      <td>('This photo shows a Pakistani provincial chie...</td>\n",
       "      <td>Photo shows Syed Murad Ali Shah, chief ministe...</td>\n",
       "      <td>Photo shows Syed Murad Ali Shah, chief ministe...</td>\n",
       "      <td>eng</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  fact_check_id                                                ocr  \\\n",
       "0  10719.0        84382.0                                                 []   \n",
       "1  21478.0        56588.0  [('TN Todo Noticias [USER] Noticias ULTIMO MOM...   \n",
       "2  14336.0       103924.0  [('ان شهادت ه امام من شریک تمبوتراب اسکاوٹ', '...   \n",
       "\n",
       "                                                text  \\\n",
       "0  ('Fue en 1908. Los belgas leyendo la Biblia an...   \n",
       "1  ('Y seguimos sumando muertes... [URL]', 'And w...   \n",
       "2  ('Masjid to carona ha lakin yha nae ha carona ...   \n",
       "\n",
       "                                        ocr_original  \\\n",
       "0                                               None   \n",
       "1  TN Todo Noticias [USER] Noticias ULTIMO MOMENT...   \n",
       "2            ان شهادت ه امام من شریک تمبوتراب اسکاوٹ   \n",
       "\n",
       "                                      ocr_translated ocr_language  \\\n",
       "0                                               None         None   \n",
       "1  TN All News [USER] News LAST MOMENT The journa...          spa   \n",
       "2     that testimony My Imam Tembutrab Scout partner          urd   \n",
       "\n",
       "                                       text_original  \\\n",
       "0  Fue en 1908. Los belgas leyendo la Biblia ante...   \n",
       "1                Y seguimos sumando muertes... [URL]   \n",
       "2  Masjid to carona ha lakin yha nae ha carona ku...   \n",
       "\n",
       "                                     text_translated text_language  \\\n",
       "0  It was in 1908. Belgians reading the Bible bef...           spa   \n",
       "1                 And we keep adding deaths... [URL]           spa   \n",
       "2  Masjid to carona ha lakin yha nae ha carona ku...           hin   \n",
       "\n",
       "   text_confidence                                              claim  \\\n",
       "0              1.0  ('Los belgas leyendo la Biblia antes de colgar...   \n",
       "1              1.0  ('Falleció el periodista Carlos Ferrara, que h...   \n",
       "2              1.0  ('Photo shows Syed Murad Ali Shah, chief minis...   \n",
       "\n",
       "                                               title  \\\n",
       "0  ('Según historiadores, es improbable que la fo...   \n",
       "1  ('El periodista argentino Carlos Ferrara no mu...   \n",
       "2  ('This photo shows a Pakistani provincial chie...   \n",
       "\n",
       "                                      original_claim  \\\n",
       "0  Los belgas leyendo la Biblia antes de colgar a...   \n",
       "1  Falleció el periodista Carlos Ferrara, que hab...   \n",
       "2  Photo shows Syed Murad Ali Shah, chief ministe...   \n",
       "\n",
       "                                    translated_claim language  label  \n",
       "0  Belgians reading the Bible before hanging a 7-...      spa      1  \n",
       "1  Journalist Carlos Ferrara died, who had fainte...      spa      1  \n",
       "2  Photo shows Syed Murad Ali Shah, chief ministe...      eng      1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factcheck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
